{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "bibliography: bio.bib\n",
        "csl: harvard-cite-them-right.csl\n",
        "title: NA_Group2's Group Project\n",
        "assess:\n",
        "  group-date: '2024-12-17'\n",
        "execute:\n",
        "  echo: false\n",
        "  freeze: true\n",
        "format:\n",
        "  html:\n",
        "    code-copy: true\n",
        "    code-link: true\n",
        "    toc: true\n",
        "    toc-title: On this page\n",
        "    toc-depth: 2\n",
        "    toc_float:\n",
        "      collapsed: false\n",
        "      smooth_scroll: true\n",
        "  pdf:\n",
        "    include-in-header:\n",
        "      text: |\n",
        "        \\addtokomafont{disposition}{\\rmfamily}\n",
        "    mainfont: Spectral\n",
        "    sansfont: Roboto Flex\n",
        "    monofont: Liberation Mono\n",
        "    papersize: a4\n",
        "    geometry:\n",
        "      - top=20mm\n",
        "      - left=25mm\n",
        "      - right=25mm\n",
        "      - bottom=20mm\n",
        "      - heightrounded\n",
        "    toc: false\n",
        "    number-sections: false\n",
        "    colorlinks: true\n",
        "    highlight-style: github\n",
        "jupyter:\n",
        "  jupytext:\n",
        "    text_representation:\n",
        "      extension: .qmd\n",
        "      format_name: quarto\n",
        "      format_version: '1.0'\n",
        "      jupytext_version: 1.16.4\n",
        "  kernelspec:\n",
        "    display_name: Python (base)\n",
        "    language: python\n",
        "    name: base\n",
        "---"
      ],
      "id": "f5c66a66"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "import os\n",
        "import pandas as pd"
      ],
      "id": "76aaeba3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "host = 'https://orca.casa.ucl.ac.uk'\n",
        "path = '~jreades/data'\n",
        "file = '20240614-London-listings.parquet'\n",
        "\n",
        "if os.path.exists(file):\n",
        "  df = pd.read_parquet(file)\n",
        "else: \n",
        "  df = pd.read_parquet(f'{host}/{path}/{file}')\n",
        "  df.to_parquet(file)"
      ],
      "id": "361bb021",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Declaration of Authorship {.unnumbered .unlisted}\n",
        "\n",
        "We, [NA_Group2], pledge our honor that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.\n",
        "\n",
        "Date: 17/12/2024\n",
        "\n",
        "Student Numbers: 4\n",
        "\n",
        "\n",
        "## Declaration of Large Language Model Use\n",
        "\n",
        "In this report, Large Language Models, specifically OpenAI's ChatGPT and Anthropic's Claude, were used as supplementary tools. Their usage was managed to ensure the integrity, originality, and quality of the report. The specific ways in which the LLMs contributed are as follows:  \n",
        "\n",
        "1. **Language Enhancement**: The LLMs were used to improve the grammar, sentence structure, readability, and clarity of the text.\n",
        "  \n",
        "2. **Assistance with Code and Data Analysis**: In sections involving programming or data analysis, the LLMs provided guidance on debugging, refining code, optimizing algorithms, and suggesting alternative approaches to analytical problems.\n",
        "\n",
        "3. **Concept Clarification**: For technical or theoretical components, the LLMs served as a resource to clarify complex concepts, provide background explanations, and ensure accuracy in the presentation of ideas.\n",
        "\n",
        "\n",
        "## Word Count \n",
        "\n",
        "**Text Content**: 1,727\n",
        "\n",
        "**Visuals**: 6 figures × 150 per figure = 900 \n",
        "\n",
        "**Total**: 2,627\n",
        "\n",
        "\n",
        "{{< pagebreak >}}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Brief Group Reflection\n",
        "\n",
        "| What Went Well                                      | What Was Challenging                               |  \n",
        "|----------------------------------------------------|--------------------------------------------------|  \n",
        "| A clear focus on the 90-day policy as the core theme.| InsideAirbnb lacks direct data on annual rental days, requiring estimation. |  \n",
        "| Initial observations revealed significant policy violations.| Snapshot data is time-sensitive and may not fully capture temporal changes. |  \n",
        "| Spatial clustering analysis highlighted policy-violating hotspots.| The logistic regression model had low explanatory power (R² = 0.05). |  \n",
        "| Iterative refinement of models improved insights (GWR R² = 0.55–0.96).| Residual analysis and Moran's I indicated spatial heterogeneity. |  \n",
        "| Identified policy violations' impact on rent and vacancy rates.| Incorporating new variables (e.g., attraction density) was computationally intensive. |  \n",
        "| Combined four quarters of snapshot data to mitigate time-sensitivity limitations.| Assessing the broader socioeconomic impact was limited by data availability. |\n",
        "\n",
        "## Priorities for Feedback\n",
        "\n",
        "**Code Consistency:** When working together, our code and methods often feel disconnected and lack coherence. How can we ensure clearer, more consistent approaches across the team?\n",
        "\n",
        "**Accuracy of Policy Violation Estimates:** Given the limitations of InsideAirbnb data (e.g., no direct measure for annual rental days), we used proxies like reviews and minimum stay to estimate violations. We’d appreciate guidance on how to validate or improve this estimation method to reduce bias.\n",
        "\n",
        "**Model Discrepancy and Improvement:** While GWR significantly improved explanatory power (R² = 0.55–0.96), the initial logistic regression model had very low explanatory power (R² = 0.05). We would appreciate feedback on whether there are key factors missing in the initial model and how to avoid potential overfitting or over-reliance on local weights in the GWR model.\n",
        "\n",
        "\n",
        "{{< pagebreak >}}\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## 1. Who collected the InsideAirbnb data?\n",
        "\n",
        "\n",
        "The data was collected, integrated, and published by Inside Airbnb’s policy and housing researchers, led by founder Murray Cox, an activist using data-driven insights to address housing challenges [@InsideAirbnb].\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Why did they collect the InsideAirbnb data?\n",
        "\n",
        "\n",
        "Inside Airbnb collects data to help communities understand Airbnb's impact on housing and neighborhoods. They aim to provide data that empowers communities to make informed decisions, manage short-term rentals, protect residents from negative effects like rising rents and housing shortages, and support collective efforts by residents and activists [@cox_inside_2023].\n",
        "\n",
        "---\n",
        "\n",
        "## 3. How did they collect it?\n",
        "\n",
        "\n",
        "Inside Airbnb used Python-based web scraping to collect publicly available data from the Airbnb website [@alsudais_incorrect_2021]. The data is captured as a time-specific snapshot, including availability calendars (up to 365 days), reviews, names, photos, and other publicly visible details. After collection, the data was cleaned, analyzed, and aggregated. Airbnb’s categorization of unavailable nights (booked or blocked) and location anonymization were retained unchanged, while neighborhood names were refined using city-defined boundaries for accuracy [@InsideAirbnb].\n",
        "\n",
        "---\n",
        "\n",
        "## 4. How does the method of collection (Q3) impact the completeness and/or accuracy of the InsideAirbnb data? How well does it represent the process it seeks to study, and what wider issues does this raise?\n",
        "\n",
        "\n",
        "The Inside Airbnb data reveals key market trends and geographic distribution of listings, offering a foundation for analyzing short-term rental impacts on residential neighborhoods. But it has the following limitations:  \n",
        "\n",
        "1. The data includes only Airbnb listings, which may be removed or hidden by Airbnb due to business or regulatory pressures. For example, in 2015, Airbnb deleted over 1,000 entire-home listings in New York [@cox_how_2016].\n",
        "\n",
        "2. The data's snapshot nature cannot reflect ongoing changes, such as newly added or removed listings. Differences between dataset versions can also affect reliability and reproducibility if the version used is not specified.\n",
        "\n",
        "3. The data excludes off-platform bookings, such as private arrangements to avoid platform fees or bypass rental day regulations. Additionally, booked dates are marked as unavailable, creating the false impression that popular listings are unrentable. These factors lead to an underestimation of actual bookings and occupancy rates.\n",
        "\n",
        "4. Geolocation data is anonymized, shifting locations by up to 150 meters and scattering listings within buildings, which reduces accuracy for neighborhood-level analysis.\n",
        "\n",
        "5. Airbnb sometimes assigns the same ID to both 'Experiences' and 'Listings,' causing 'Experience' reviews to be misclassified as 'Listing' reviews and leading to inaccuracies in review data [@alsudais_incorrect_2021].\n",
        "\n",
        "---\n",
        "\n",
        "## 5. What ethical considerations does the use of the InsideAirbnb data raise? \n",
        "\n",
        "\n",
        "1. **Preventing Misuse of Data for Commercial Gain**  \n",
        "    Inside Airbnb’s data is meant to reveal the impacts of short-term rentals and to support local communities. However, malicious exploitation of the data—such as investors using it to identify high-profit areas for short-term rentals—could lead to negative consequences, including increased property purchases for rentals, displacement of residents, and worsening housing crises. This undermines the platform’s mission and further harms vulnerable groups. Researchers and policymakers have a responsibility to ensure that their use of the data aligns with community interests and to avoid sharing data in ways that enable harmful commercial use.\n",
        "\n",
        "2. **Protecting User Privacy**  \n",
        "    Since the data is collected through web scraping, Airbnb hosts and guests may not have given consent for its use. Even anonymized geographic data can unintentionally expose sensitive information, especially in small communities or for standalone properties. Publicly identifying short-term rental activity could subject hosts and guests to harassment, discrimination, or financial loss. Research outputs should generalize findings to avoid unintentionally identifying individuals and use neutral language to prevent stigmatization of particular groups.\n",
        "\n",
        "3. **Ensuring Representation of Vulnerable Communities: Promoting Equity in Data Interpretation**  \n",
        "    While Inside Airbnb data is openly available, the ability to analyze and interpret it is often limited to technically skilled individuals, such as researchers or local activists. This can result in policies that prioritize the perspectives of these groups while neglecting the needs of underrepresented or vulnerable communities. To address this imbalance, researchers and policymakers should engage marginalized communities in the interpretation process, present findings in accessible formats, and ensure data-driven decisions promote social equity, as highlighted in Data Feminism [@dignazio_data_2020].\n",
        "\n",
        "---\n",
        "\n",
        "## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and the types of properties that they list suggest about the nature of Airbnb lettings in London? \n"
      ],
      "id": "cac7b916"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "#Reading airbnb data 2013.12\n",
        "import pandas as pd  \n",
        "import numpy as np  \n",
        "from scipy.stats import pearsonr, pointbiserialr, chi2_contingency  \n",
        "import matplotlib.pyplot as plt  \n",
        "from matplotlib.font_manager import FontProperties  \n",
        "from matplotlib.colors import LinearSegmentedColormap  \n",
        "from matplotlib.gridspec import GridSpec  \n",
        "import seaborn as sns  \n",
        "import os  # For creating directories\n",
        "\n",
        "# loading data\n",
        "file_path = 'data/listings_202312.csv' \n",
        "airbnb_data = pd.read_csv(file_path)  \n",
        " \n",
        "print(airbnb_data.info()) \n",
        "\n",
        "#create plots directory\n",
        "output_dir = \"plots\"  \n",
        "if not os.path.exists(output_dir):  \n",
        "    os.makedirs(output_dir) "
      ],
      "id": "a01837fa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "# estimate booking days per year by reviews and minimum nights\n",
        "airbnb_data = airbnb_data[airbnb_data['availability_365'] > 0] \n",
        "airbnb_data['estimated_nights_booked'] = airbnb_data['reviews_per_month'] * 12 * airbnb_data['minimum_nights'] * 2  \n",
        "\n",
        "#Data cleaning\n",
        "# Replace NaN with 0\n",
        "airbnb_data['estimated_nights_booked'] = airbnb_data['estimated_nights_booked'].fillna(0)\n",
        "# Convert the column to integers\n",
        "airbnb_data['estimated_nights_booked'] = airbnb_data['estimated_nights_booked'].astype(int)\n",
        "\n",
        "airbnb_data.to_csv('data/processed_airbnb_data.csv', index=False)  "
      ],
      "id": "9357b997",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### The 90-Day Rule and Airbnb Commercialization  \n",
        "\n",
        "Airbnb has reduced long-term rentals, raised rents, and driven commercialization in London’s housing market. The 90-day rule limits entire-home short-term rentals to 90 days per year without special planning permission [@greaterlondonauthority], but enforcement depends on self-reporting, which is weak. To address this, we use InsideAirbnb data to analyze booking patterns, host behavior, and property types to assess commercialization and compliance risks. Let's start by conducting an initial observation of the data:\n"
      ],
      "id": "1afac7f9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os  \n",
        "import matplotlib.pyplot as plt  \n",
        "from matplotlib.gridspec import GridSpec  \n",
        "import seaborn as sns  \n",
        "\n",
        "# -------------------------------  \n",
        "# 1. Setup the Main Figure Layout  \n",
        "# -------------------------------  \n",
        "fig = plt.figure(figsize=(16, 10))  # 设置主画布大小  \n",
        "gs = GridSpec(1, 1, figure=fig)  # 使用单个网格定义地图大区域  \n",
        "\n",
        "# 定义子图位置和大小参数  \n",
        "x_offset = 0.75  \n",
        "y_offsets = [0.82, 0.62, 0.42, 0.22]  # 每个子图纵向位置  \n",
        "subplot_width, subplot_height = 0.18, 0.12  \n",
        "\n",
        "# -------------------------------  \n",
        "# 2. Plot the Map of Airbnb Listings  \n",
        "# -------------------------------  \n",
        "# 调整地图的位置，使其垂直上移  \n",
        "ax_map = fig.add_subplot(gs[0, 0])  # 地图位置  \n",
        "borough_map.plot(ax=ax_map, color=\"lightblue\", edgecolor=\"darkblue\", alpha=0.44)  \n",
        "\n",
        "# 定义房间类型颜色  \n",
        "room_type_colors = {  \n",
        "    \"Entire home/apt\": \"#FF7F7F\",  \n",
        "    \"Private room\": \"#77DD77\",  \n",
        "    \"Shared room\": \"#FDFD96\",  \n",
        "    \"Hotel room\": \"#FFA07A\",  \n",
        "}  \n",
        "\n",
        "# 绘制各房型分布点  \n",
        "for room_type, color in room_type_colors.items():  \n",
        "    gdf[gdf[\"room_type\"] == room_type].plot(  \n",
        "        ax=ax_map, color=color, markersize=2, label=room_type, alpha=0.7  \n",
        "    )  \n",
        "\n",
        "# 添加图例  \n",
        "legend = ax_map.legend(  \n",
        "    title=\"Room Type\", loc=\"upper left\", fontsize=10, frameon=True, edgecolor=\"lightgray\"  \n",
        ")  \n",
        "legend.get_title().set_fontsize(10)  \n",
        "legend.get_title().set_fontweight(\"bold\")  \n",
        "\n",
        "# 添加标题  \n",
        "ax_map.set_title(\"Airbnb Listings in London\", fontsize=13, pad=15)  # 标题位置和距离  \n",
        "\n",
        "# 清理坐标轴  \n",
        "ax_map.set_xticks([]), ax_map.set_yticks([])  \n",
        "for spine in ax_map.spines.values():  \n",
        "    spine.set_visible(False)  \n",
        "\n",
        "# -------------------------------  \n",
        "# 3. Subplot 1: Estimated Booking Days  \n",
        "# -------------------------------  \n",
        "ax1 = fig.add_axes([x_offset, y_offsets[0], subplot_width, subplot_height])  # 第一子图位置  \n",
        "bins = [0, 30, 60, 89.999, 120, 150, 180, 210, 240, 270, 300, 330, 356]  \n",
        "labels = [  \n",
        "    \"0-30\",  \n",
        "    \"30-60\",  \n",
        "    \"60-90\",  \n",
        "    \"90-120\",  \n",
        "    \"120-150\",  \n",
        "    \"150-180\",  \n",
        "    \"180-210\",  \n",
        "    \"210-240\",  \n",
        "    \"240-270\",  \n",
        "    \"270-300\",  \n",
        "    \"300-330\",  \n",
        "    \"330-356\",  \n",
        "    \"356+\",  \n",
        "]  \n",
        "\n",
        "sns.histplot(  \n",
        "    data=data,  \n",
        "    x=\"estimated_nights_booked\",  \n",
        "    hue=\"booking_color\",  \n",
        "    palette={\"blue\": \"#6A9FB5\", \"red\": \"#FF6F61\"},  \n",
        "    multiple=\"dodge\",  \n",
        "    edgecolor=\"black\",  \n",
        "    linewidth=0.5,  \n",
        "    binwidth=15,  \n",
        "    ax=ax1,  \n",
        ")  \n",
        "ax1.set_title(\"1. Estimated Booking Days\", fontsize=10, fontweight=\"bold\")  \n",
        "ax1.set_xlabel(\"Booking Days (0-356)\", fontsize=8)  \n",
        "ax1.set_ylabel(\"Listings\", fontsize=8)  \n",
        "ax1.tick_params(axis=\"x\", rotation=45, labelsize=7)  \n",
        "ax1.tick_params(axis=\"y\", labelsize=7)  \n",
        "ax1.set_xlim(0, 356)  \n",
        "ax1.axvline(x=90, color=\"black\", linestyle=\"--\", linewidth=1)  \n",
        "ax1.set_xticks(bins)  \n",
        "ax1.set_xticklabels(labels, rotation=45, fontsize=6.5)  \n",
        "\n",
        "# 添加自定义图例  \n",
        "ax1.legend(  \n",
        "    handles=[  \n",
        "        plt.Line2D([0], [0], color=\"#6A9FB5\", lw=4, label=\"< 90\"),  \n",
        "        plt.Line2D([0], [0], color=\"#FF6F61\", lw=4, label=\">=90\"),  \n",
        "    ],  \n",
        "    title=\"Booking Days\",  \n",
        "    loc=\"upper right\",  \n",
        "    fontsize=8,  \n",
        "    title_fontsize=8,  \n",
        ")  \n",
        "ax1.spines[\"top\"].set_visible(False)  \n",
        "\n",
        "# -------------------------------  \n",
        "# 4. Subplot 2: Listings per Host  \n",
        "# -------------------------------  \n",
        "ax2 = fig.add_axes([x_offset, y_offsets[1], subplot_width, subplot_height])  # 第二子图位置  \n",
        "sns.histplot(  \n",
        "    listings_per_host,  \n",
        "    kde=False,  \n",
        "    color=\"lightblue\",  \n",
        "    edgecolor=\"black\",  \n",
        "    linewidth=0.5,  \n",
        "    binwidth=0.8,  \n",
        "    ax=ax2,  \n",
        ")  \n",
        "ax2.set_title(\"2. Listings per Host\", fontsize=10, fontweight=\"bold\")  \n",
        "ax2.set_xlabel(\"Number of Listings\", fontsize=8)  \n",
        "ax2.set_ylabel(\"Hosts\", fontsize=8)  \n",
        "ax2.tick_params(axis=\"x\", labelsize=7)  \n",
        "ax2.tick_params(axis=\"y\", labelsize=7)  \n",
        "ax2.set_xticks(range(1, 12))  \n",
        "ax2.set_xticklabels([str(i) for i in range(1, 11)] + [\"10+\"])  \n",
        "ax2.set_xlim(0.5, 11.5)  \n",
        "ax2.spines[\"top\"].set_visible(False)  \n",
        "ax2.spines[\"right\"].set_visible(False)  \n",
        "\n",
        "# -------------------------------  \n",
        "# 5. Subplot 3: Room Type Count  \n",
        "# -------------------------------  \n",
        "ax3 = fig.add_axes([x_offset, y_offsets[2], subplot_width, subplot_height])  # 第三子图位置  \n",
        "room_type_count = data[\"room_type\"].value_counts()  \n",
        "room_type_count.index = [\"Entire home\", \"Private room\", \"Shared room\", \"Hotel room\"]  \n",
        "sns.barplot(  \n",
        "    x=room_type_count.index,  \n",
        "    y=room_type_count.values,  \n",
        "    palette=[\"#FF7F7F\", \"#AEC6CF\", \"#FDFD96\", \"#FFA07A\"],  \n",
        "    edgecolor=\"black\",  \n",
        "    linewidth=0.5,  \n",
        "    ax=ax3,  \n",
        ")  \n",
        "ax3.set_title(\"3. Room Type Count\", fontsize=10, fontweight=\"bold\")  \n",
        "ax3.set_xlabel(\"Room Type\", fontsize=8)  \n",
        "ax3.set_ylabel(\"Count\", fontsize=8)  \n",
        "ax3.tick_params(axis=\"x\", labelsize=7)  \n",
        "ax3.tick_params(axis=\"y\", labelsize=7)  \n",
        "ax3.spines[\"top\"].set_visible(False)  \n",
        "ax3.spines[\"right\"].set_visible(False)  \n",
        "\n",
        "# -------------------------------  \n",
        "# 6. Subplot 4: Minimum Nights Distribution  \n",
        "# -------------------------------  \n",
        "ax4 = fig.add_axes([x_offset, y_offsets[3], subplot_width, subplot_height])  # 第四子图位置  \n",
        "sns.histplot(  \n",
        "    data[\"minimum_nights\"],  \n",
        "    bins=1000,  \n",
        "    kde=False,  \n",
        "    color=\"lightcoral\",  \n",
        "    edgecolor=\"black\",  \n",
        "    linewidth=0.5,  \n",
        "    ax=ax4,  \n",
        ")  \n",
        "ax4.set_title(\"4. Minimum Nights Distribution\", fontsize=10, fontweight=\"bold\")  \n",
        "ax4.set_xlabel(\"Minimum Nights\", fontsize=8)  \n",
        "ax4.set_ylabel(\"Listings\", fontsize=8)  \n",
        "ax4.tick_params(axis=\"x\", labelsize=7)  \n",
        "ax4.tick_params(axis=\"y\", labelsize=7)  \n",
        "ax4.set_xlim(0, 40)  \n",
        "ax4.spines[\"top\"].set_visible(False)  \n",
        "ax4.spines[\"right\"].set_visible(False)  \n",
        "\n",
        "# -------------------------------  \n",
        "# 7. Save and Display the Plot  \n",
        "# -------------------------------  \n",
        "os.makedirs(\"plots\", exist_ok=True)  \n",
        "plt.savefig(  \n",
        "    \"plots/figure_1_Airbnb_Listings_in_London_optimized.png\", dpi=600, bbox_inches=\"tight\"  \n",
        ")  \n",
        "plt.show()"
      ],
      "id": "0895df8b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "# -------------------------------  \n",
        "# 1. Setup the Main Figure Layout  \n",
        "# -------------------------------  \n",
        "fig = plt.figure(figsize=(16, 10))  # 设置主画布大小  \n",
        "gs = GridSpec(1, 1, figure=fig)  # 使用单个网格定义地图大区域  \n",
        "\n",
        "# 定义子图位置和大小参数  \n",
        "x_offset = 0.75  \n",
        "y_offsets = [0.75, 0.55, 0.35, 0.15]  # 将所有子图垂直偏移向下（原来是 [0.82, 0.62, 0.42, 0.22]）  \n",
        "subplot_width, subplot_height = 0.18, 0.12  \n",
        "\n",
        "# -------------------------------  \n",
        "# 2. Plot the Map of Airbnb Listings  \n",
        "# -------------------------------  \n",
        "ax_map = fig.add_subplot(gs[0, 0])  # 地图位置  \n",
        "borough_map.plot(ax=ax_map, color=\"lightblue\", edgecolor=\"darkblue\", alpha=0.44)  \n",
        "\n",
        "# Define room type colors  \n",
        "room_type_colors = {  \n",
        "    \"Entire home/apt\": \"#FF7F7F\",  \n",
        "    \"Private room\": \"#77DD77\",  \n",
        "    \"Shared room\": \"#FDFD96\",  \n",
        "    \"Hotel room\": \"#FFA07A\",  \n",
        "}  \n",
        "\n",
        "# Plot Airbnb listings by room type  \n",
        "for room_type, color in room_type_colors.items():  \n",
        "    gdf[gdf[\"room_type\"] == room_type].plot(  \n",
        "        ax=ax_map, color=color, markersize=2, label=room_type, alpha=0.7  \n",
        "    )  \n",
        "\n",
        "# Add legend  \n",
        "legend = ax_map.legend(  \n",
        "    title=\"Room Type\", loc=\"upper left\", fontsize=10, frameon=True, edgecolor=\"lightgray\"  \n",
        ")  \n",
        "legend.get_title().set_fontsize(10)  \n",
        "legend.get_title().set_fontweight(\"bold\")  \n",
        "\n",
        "# Add map title  \n",
        "ax_map.set_title(\"Airbnb Listings in London\", fontsize=13, pad=15)  # 标题位置和距离  \n",
        "\n",
        "# Clean up the map axes  \n",
        "ax_map.set_xticks([]), ax_map.set_yticks([])  \n",
        "for spine in ax_map.spines.values():  \n",
        "    spine.set_visible(False)  \n",
        "\n",
        "# -------------------------------  \n",
        "# 3. Subplot 1: Estimated Booking Days  \n",
        "# -------------------------------  \n",
        "ax1 = fig.add_axes([x_offset, y_offsets[0], subplot_width, subplot_height])  # 第一子图位置  \n",
        "bins = [0, 30, 60, 89.999, 120, 150, 180, 210, 240, 270, 300, 330, 356]  \n",
        "labels = [  \n",
        "    \"0-30\",  \n",
        "    \"30-60\",  \n",
        "    \"60-90\",  \n",
        "    \"90-120\",  \n",
        "    \"120-150\",  \n",
        "    \"150-180\",  \n",
        "    \"180-210\",  \n",
        "    \"210-240\",  \n",
        "    \"240-270\",  \n",
        "    \"270-300\",  \n",
        "    \"300-330\",  \n",
        "    \"330-356\",  \n",
        "    \"356+\",  \n",
        "]  \n",
        "\n",
        "sns.histplot(  \n",
        "    data=data,  \n",
        "    x=\"estimated_nights_booked\",  \n",
        "    hue=\"booking_color\",  \n",
        "    palette={\"blue\": \"#6A9FB5\", \"red\": \"#FF6F61\"},  \n",
        "    multiple=\"dodge\",  \n",
        "    edgecolor=\"black\",  \n",
        "    linewidth=0.5,  \n",
        "    binwidth=15,  \n",
        "    ax=ax1,  \n",
        ")  \n",
        "ax1.set_title(\"1. Estimated Booking Days\", fontsize=10, fontweight=\"bold\")  \n",
        "ax1.set_xlabel(\"Booking Days (0-356)\", fontsize=8)  \n",
        "ax1.set_ylabel(\"Listings\", fontsize=8)  \n",
        "ax1.tick_params(axis=\"x\", rotation=45, labelsize=7)  \n",
        "ax1.tick_params(axis=\"y\", labelsize=7)  \n",
        "ax1.set_xlim(0, 356)  \n",
        "ax1.axvline(x=90, color=\"black\", linestyle=\"--\", linewidth=1)  \n",
        "ax1.set_xticks(bins)  \n",
        "ax1.set_xticklabels(labels, rotation=45, fontsize=6.5)  \n",
        "\n",
        "# Add custom legend  \n",
        "ax1.legend(  \n",
        "    handles=[  \n",
        "        plt.Line2D([0], [0], color=\"#6A9FB5\", lw=4, label=\"< 90\"),  \n",
        "        plt.Line2D([0], [0], color=\"#FF6F61\", lw=4, label=\">=90\"),  \n",
        "    ],  \n",
        "    title=\"Booking Days\",  \n",
        "    loc=\"upper right\",  \n",
        "    fontsize=8,  \n",
        "    title_fontsize=8,  \n",
        ")  \n",
        "ax1.spines[\"top\"].set_visible(False)  \n",
        "\n",
        "# -------------------------------  \n",
        "# 4. Subplot 2: Listings per Host  \n",
        "# -------------------------------  \n",
        "ax2 = fig.add_axes([x_offset, y_offsets[1], subplot_width, subplot_height])  # 第二子图位置  \n",
        "sns.histplot(  \n",
        "    listings_per_host,  \n",
        "    kde=False,  \n",
        "    color=\"lightblue\",  \n",
        "    edgecolor=\"black\",  \n",
        "    linewidth=0.5,  \n",
        "    binwidth=0.8,  \n",
        "    ax=ax2,  \n",
        ")  \n",
        "ax2.set_title(\"2. Listings per Host\", fontsize=10, fontweight=\"bold\")  \n",
        "ax2.set_xlabel(\"Number of Listings\", fontsize=8)  \n",
        "ax2.set_ylabel(\"Hosts\", fontsize=8)  \n",
        "ax2.tick_params(axis=\"x\", labelsize=7)  \n",
        "ax2.tick_params(axis=\"y\", labelsize=7)  \n",
        "ax2.set_xticks(range(1, 12))  \n",
        "ax2.set_xticklabels([str(i) for i in range(1, 11)] + [\"10+\"])  \n",
        "ax2.set_xlim(0.5, 11.5)  \n",
        "ax2.spines[\"top\"].set_visible(False)  \n",
        "ax2.spines[\"right\"].set_visible(False)  \n",
        "\n",
        "# -------------------------------  \n",
        "# 5. Subplot 3: Room Type Count  \n",
        "# -------------------------------  \n",
        "ax3 = fig.add_axes([x_offset, y_offsets[2], subplot_width, subplot_height])  # 第三子图位置  \n",
        "room_type_count = data[\"room_type\"].value_counts()  \n",
        "room_type_count.index = [\"Entire home\", \"Private room\", \"Shared room\", \"Hotel room\"]  \n",
        "sns.barplot(  \n",
        "    x=room_type_count.index,  \n",
        "    y=room_type_count.values,  \n",
        "    palette=[\"#FF7F7F\", \"#AEC6CF\", \"#FDFD96\", \"#FFA07A\"],  \n",
        "    edgecolor=\"black\",  \n",
        "    linewidth=0.5,  \n",
        "    ax=ax3,  \n",
        ")  \n",
        "ax3.set_title(\"3. Room Type Count\", fontsize=10, fontweight=\"bold\")  \n",
        "ax3.set_xlabel(\"Room Type\", fontsize=8)  \n",
        "ax3.set_ylabel(\"Count\", fontsize=8)  \n",
        "ax3.tick_params(axis=\"x\", labelsize=7)  \n",
        "ax3.tick_params(axis=\"y\", labelsize=7)  \n",
        "ax3.spines[\"top\"].set_visible(False)  \n",
        "ax3.spines[\"right\"].set_visible(False)  \n",
        "\n",
        "# -------------------------------  \n",
        "# 6. Subplot 4: Minimum Nights Distribution  \n",
        "# -------------------------------  \n",
        "ax4 = fig.add_axes([x_offset, y_offsets[3], subplot_width, subplot_height])  # 第四子图位置  \n",
        "sns.histplot(  \n",
        "    data[\"minimum_nights\"],  \n",
        "    bins=1000,  \n",
        "    kde=False,  \n",
        "    color=\"lightcoral\",  \n",
        "    edgecolor=\"black\",  \n",
        "    linewidth=0.5,  \n",
        "    ax=ax4,  \n",
        ")  \n",
        "ax4.set_title(\"4. Minimum Nights Distribution\", fontsize=10, fontweight=\"bold\")  \n",
        "ax4.set_xlabel(\"Minimum Nights\", fontsize=8)  \n",
        "ax4.set_ylabel(\"Listings\", fontsize=8)  \n",
        "ax4.tick_params(axis=\"x\", labelsize=7)  \n",
        "ax4.tick_params(axis=\"y\", labelsize=7)  \n",
        "ax4.set_xlim(0, 40)  \n",
        "ax4.spines[\"top\"].set_visible(False)  \n",
        "ax4.spines[\"right\"].set_visible(False)  \n",
        "\n",
        "# -------------------------------  \n",
        "# 7. Save and Display the Plot  \n",
        "# -------------------------------  \n",
        "os.makedirs(\"plots\", exist_ok=True)  \n",
        "plt.savefig(  \n",
        "    \"plots/figure_1_Airbnb_Listings_in_London_optimized.png\", dpi=600, bbox_inches=\"tight\"  \n",
        ")  \n",
        "\n",
        "# -------------------------------  \n",
        "plt.subplots_adjust(bottom=0.05)  \n",
        "fig.text(0.5, 0.01, 'Figure 1: Airbnb Listings Analysis in London',   \n",
        "         ha='center', fontsize=18, fontweight='bold')\n",
        "\n",
        "plt.show()"
      ],
      "id": "0d5a9645",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Key Findings  \n",
        "\n",
        "1. **Spatial Hotspot**:  \n",
        "    Central London has the highest density of short-term rentals, with entire homes dominating high-demand, high-violation areas of the 90-day rule (Figure 1, Map).\n",
        "\n",
        "2. **Frequent Breaches of the 90-Day Limit**:  \n",
        "    Booking data, based on reviews, pricing, and minimum nights [@insideairbnb2023], shows entire homes and hotel-like properties often exceed the 90-day limit, especially in commercialized areas (Figure 1, Panel 1). \n",
        "\n",
        "To address the limitations of using only snapshot data, we merged datasets from December 2023 to September 2024 and removed duplicates to ensure greater accuracy:\n"
      ],
      "id": "131dc42e"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cache": true
      },
      "source": [
        "#| echo: false\n",
        "\n",
        "\n",
        "# -------------------------------  \n",
        "# 1. Load and Combine Data  \n",
        "# -------------------------------  \n",
        "# Define the folder and file names  \n",
        "data_folder = \"data\"  \n",
        "file_names = [\"listings_20243.csv\", \"listings_20246.csv\", \"listings_20249.csv\"]  \n",
        "\n",
        "# Load and combine the datasets  \n",
        "data_list = [pd.read_csv(os.path.join(data_folder, file)) for file in file_names]  \n",
        "combined_data = pd.concat(data_list, ignore_index=True)  \n",
        "\n",
        "# Combine with airbnb_data (2023 data)  \n",
        "all_data = pd.concat([combined_data, airbnb_data], ignore_index=True)  \n",
        "\n",
        "# -------------------------------  \n",
        "# 2. Data Cleaning  \n",
        "# -------------------------------  \n",
        "# Filter out listings with no availability  \n",
        "all_data = all_data[all_data['availability_365'] > 0]  \n",
        "\n",
        "# Calculate estimated booking days  \n",
        "all_data['estimated_nights_booked'] = (  \n",
        "    all_data['reviews_per_month'] * 12 * all_data['minimum_nights'] * 2  \n",
        ")  \n",
        "all_data['estimated_nights_booked'] = all_data['estimated_nights_booked'].fillna(0).astype(int)  \n",
        "\n",
        "# Remove duplicates  \n",
        "all_data = all_data.drop_duplicates()  \n",
        "\n",
        "# -------------------------------  \n",
        "# 3. Create 'host_type' Column  \n",
        "# -------------------------------  \n",
        "# Ensure 'calculated_host_listings_count' exists  \n",
        "if 'calculated_host_listings_count' not in all_data.columns:  \n",
        "    raise KeyError(\"'calculated_host_listings_count' column is missing from the DataFrame.\")  \n",
        "\n",
        "# Create 'host_type' column  \n",
        "all_data['host_type'] = all_data['calculated_host_listings_count'].apply(  \n",
        "    lambda x: 'Single' if x == 1 else 'Multi'  \n",
        ")  \n",
        "\n",
        "# -------------------------------  \n",
        "# 4. Calculate Metrics  \n",
        "# -------------------------------  \n",
        "# Host type proportions  \n",
        "host_proportions = all_data['host_type'].value_counts(normalize=True) * 100  \n",
        "host_proportions = host_proportions.round(2)  \n",
        "\n",
        "# Average booking days by host type  \n",
        "host_avg_booking_days = all_data.groupby('host_type')['estimated_nights_booked'].mean().round(2)  \n",
        "\n",
        "# Room type proportions  \n",
        "room_type_proportions = all_data['room_type'].value_counts(normalize=True) * 100  \n",
        "room_type_proportions = room_type_proportions.round(2)  \n",
        "\n",
        "# Average booking days by room type  \n",
        "room_type_avg_days = all_data.groupby('room_type')['estimated_nights_booked'].mean().round(2)  \n",
        "\n",
        "# Average price and minimum nights by host type and room type  \n",
        "avg_price_heatmap = all_data.pivot_table(index='host_type', columns='room_type', values='price', aggfunc='mean').round(2)  \n",
        "min_nights_heatmap = all_data.pivot_table(index='host_type', columns='room_type', values='minimum_nights', aggfunc='mean').round(2)  "
      ],
      "id": "a62efd0a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cache": true
      },
      "source": [
        "#| echo: false\n",
        "# 5. Visualizations  \n",
        "# -------------------------------  \n",
        "fig = plt.figure(figsize=(16, 14))  \n",
        "\n",
        "# Main title \n",
        "fig.text(0.5, 0.02, 'Figure 2: Analysis of Airbnb Listings by Host and Room Types',   \n",
        "         ha='center', fontsize=16, fontweight='bold') \n",
        "# First Row: Tables  \n",
        "# Left: Host Type Table  \n",
        "ax1 = fig.add_axes([0.1, 0.7, 0.35, 0.15])  \n",
        "ax1.axis('off')  \n",
        "host_table_data = pd.DataFrame({  \n",
        "    'Host Type': host_proportions.index,  \n",
        "    'Proportion (%)': host_proportions.values,  \n",
        "    'Avg Booking Days': host_avg_booking_days.values  \n",
        "})  \n",
        "host_table = ax1.table(  \n",
        "    cellText=host_table_data.values,  \n",
        "    colLabels=host_table_data.columns,  \n",
        "    cellLoc='center',  \n",
        "    loc='center',  \n",
        "    bbox=[0, 0, 1, 1]  \n",
        ")  \n",
        "host_table.auto_set_font_size(False)  \n",
        "host_table.set_fontsize(12)  \n",
        "for key, cell in host_table.get_celld().items():  \n",
        "    if key[0] == 0:  # Header row  \n",
        "        cell.set_text_props(weight='bold')  \n",
        "        cell.set_height(0.1)  \n",
        "    else:  \n",
        "        cell.set_height(0.15)  \n",
        "\n",
        "# Add left-side title  \n",
        "fig.text(0.15, 0.87, 'Host Type Distribution and Booking Trends', fontsize=13, fontweight='bold')  \n",
        "\n",
        "# Right: Room Type Table  \n",
        "ax2 = fig.add_axes([0.55, 0.7, 0.35, 0.15])  \n",
        "ax2.axis('off')  \n",
        "room_table_data = pd.DataFrame({  \n",
        "    'Room Type': room_type_proportions.index,  \n",
        "    'Proportion (%)': room_type_proportions.values,  \n",
        "    'Avg Booking Days': room_type_avg_days.values  \n",
        "})  \n",
        "room_table = ax2.table(  \n",
        "    cellText=room_table_data.values,  \n",
        "    colLabels=room_table_data.columns,  \n",
        "    cellLoc='center',  \n",
        "    loc='center',  \n",
        "    bbox=[0, 0, 1, 1]  \n",
        ")  \n",
        "room_table.auto_set_font_size(False)  \n",
        "room_table.set_fontsize(12)  \n",
        "for key, cell in room_table.get_celld().items():  \n",
        "    if key[0] == 0:  # Header row  \n",
        "        cell.set_text_props(weight='bold')  \n",
        "        cell.set_height(0.1)  \n",
        "    else:  \n",
        "        cell.set_height(0.15)  \n",
        "\n",
        "# Add right-side title  \n",
        "fig.text(0.6, 0.87, 'Room Type Distribution and Booking Trends', fontsize=13, fontweight='bold')  \n",
        "\n",
        "# Second Row: Bar Charts  \n",
        "# Left: Host Type Bar Chart  \n",
        "ax3 = fig.add_axes([0.1, 0.45, 0.35, 0.2])  \n",
        "host_avg_booking_days.plot(kind='bar', color=sns.color_palette(\"Pastel1\")[:2], edgecolor='black', ax=ax3)  \n",
        "ax3.set_title('Average Estimated Booking Days by Host Type', fontsize=10, fontweight='bold')  \n",
        "ax3.set_xlabel('Host Type')  \n",
        "ax3.set_ylabel('Average Booking Days')  \n",
        "\n",
        "# Right: Room Type Bar Chart  \n",
        "ax4 = fig.add_axes([0.55, 0.45, 0.35, 0.2])  \n",
        "room_type_colors = sns.color_palette(\"Pastel1\")[:len(room_type_avg_days)]  \n",
        "room_type_avg_days.plot(kind='bar', color=room_type_colors, edgecolor='black', ax=ax4)  \n",
        "ax4.set_title('Average Estimated Booking Days by Room Type', fontsize=10, fontweight='bold')  \n",
        "ax4.set_xlabel('Room Type')  \n",
        "ax4.set_ylabel('Average Booking Days')  \n",
        "\n",
        "# Add dividing line  \n",
        "fig.add_axes([0.49, 0.4, 0.01, 0.5]).plot([0, 0], [0, 1], linestyle='--', color='black', linewidth=1)  \n",
        "plt.gca().axis('off')  \n",
        "\n",
        "# Third Row: Heatmaps  \n",
        "# Left: Average Price Heatmap  \n",
        "ax5 = fig.add_axes([0.1, 0.1, 0.4, 0.23])  \n",
        "sns.heatmap(avg_price_heatmap, annot=True, fmt=\".2f\", cmap=\"Blues\", cbar=True, ax=ax5)  \n",
        "ax5.set_title('Average Price by Host and Room Types', fontsize=10, fontweight='bold')  \n",
        "\n",
        "# Right: Minimum Nights Heatmap  \n",
        "slightly_higher_saturation_reds = LinearSegmentedColormap.from_list(  \n",
        "    \"slightly_higher_saturation_reds\", [\"#fff5f5\", \"#fcd9d9\", \"#f8a8a8\", \"#f47474\"]  \n",
        ")  \n",
        "ax6 = fig.add_axes([0.55, 0.1, 0.4, 0.22])  \n",
        "sns.heatmap(min_nights_heatmap, annot=True, fmt=\".2f\", cmap=slightly_higher_saturation_reds, cbar=True, ax=ax6)  \n",
        "ax6.set_title('Minimum Nights by Host and Room Types', fontsize=10, fontweight='bold')  \n",
        "\n",
        "# Save and display  \n",
        "plt.savefig('plots/figure_2_Airbnb_Listings_Analysis.png', dpi=600, bbox_inches='tight')  \n",
        "plt.show()"
      ],
      "id": "406e753f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. **Multi-Listing Hosts Dominate the Market**:  \n",
        "    Multi-listing hosts manage 62.38% of Airbnb listings with longer average booking durations than single-listing. They primarily operate entire and hotel-like properties, which are highly commercialized with shorter stays and higher prices (Figure 2, Top Left; heatmap).\n",
        "\n",
        "4. **Entire Homes are the Highest Risk**:  \n",
        "    Entire homes (66.97% of listings) have the highest average booking days and are most likely to breach the 90-day rule, followed by hotel-like properties with short minimum stays (2.13 nights) and high commercialization (Figure 2, Top Right; heatmap).\n",
        "\n",
        "\n",
        "### Initial conclusion \n",
        "InsideAirbnb data reveals London’s highly commercialized Airbnb market with significant 90-day rule violations, concentrated in high-risk areas. Multi-listing hosts and commercialized properties are key drivers, requiring further spatial and regression analysis to assess impacts and improve enforcement.\n",
        "\n",
        "---  \n",
        "\n",
        "## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London? \n",
        "\n",
        "\n",
        "We decided to further analyze the December 2023 Inside Airbnb dataset to provide evidence on the link between Airbnb activity and policy violations. The next steps are:\n",
        "\n",
        "1. **Identifying Violation Hotspots and Impacts:**  \n",
        "    Spatial clustering will identify areas with high concentrations of 90-day rule violations. These hotspots will be analyzed for impacts on local communities, focusing on rising rents and reduced housing availability—issues tied to Airbnb-driven gentrification [@smith2006].\n",
        "\n",
        "2. **Analyzing Drivers of Violations:**  \n",
        "   Regression analysis helps identify key factors driving these violations (e.g. property type, host behavior, price, location...)\n",
        "\n",
        "This approach will highlight the need for effective regulation, and clarify enforcement priorities to enhance the efficiency of enforcement efforts.\n",
        "\n",
        "### Spatial Analysis of Policy Violations / Local Impacts  \n",
        "\n",
        "1. **Hotspot Identification:** \n",
        "We analyzed the spatial clustering of Airbnb rule-breaking properties using Moran’s I and LISA cluster maps (Figure 3). High-High clusters were found in central boroughs, such as Westminster, and eastern areas like Hackney, where violations are linked to a combination of high tourism demand, profitability of short-term rentals, and housing market pressures [@bosma2024].  \n"
      ],
      "id": "e8e3229f"
    },
    {
      "cell_type": "code",
      "metadata": {
        "cache": true
      },
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "from libpysal.weights import Queen  \n",
        "\n",
        "# Read the data  \n",
        "file_path = 'data/listings.csv'    #which is the path in this repository  \n",
        "airbnb_data = pd.read_csv(file_path)\n",
        "\n",
        "# Calculate the estimation of nights booked for each listing\n",
        "airbnb_data = airbnb_data[airbnb_data['availability_365'] > 0] \n",
        "airbnb_data['estimated_nights_booked'] = airbnb_data['reviews_per_month'] * 12 * airbnb_data['minimum_nights'] * 2 \n",
        "\n",
        "# Data cleaning: assign the estimated nights booked to each borough\n",
        "# Replace NaN with 0\n",
        "airbnb_data['estimated_nights_booked'] = airbnb_data['estimated_nights_booked'].fillna(0)\n",
        "\n",
        "# Convert the column to integers\n",
        "airbnb_data['estimated_nights_booked'] = airbnb_data['estimated_nights_booked'].astype(int)\n",
        "\n",
        "#Count the number of listings in each borough using 'neighbourhood' column\n",
        "borough_counts = airbnb_data['neighbourhood'].value_counts()\n",
        "\n",
        "# Filter the DataFrame to include only rows where estimated_nights_booked is greater than 90\n",
        "filtered_data = airbnb_data[airbnb_data['estimated_nights_booked'] > 90]\n",
        "\n",
        "#Count the number of listings with estimation of nights booked larger than 90 days in each borough\n",
        "borough_counts_90 = filtered_data['neighbourhood'].value_counts()\n",
        "\n",
        "# Merge the two series into a DataFrame\n",
        "combined_data = pd.concat([borough_counts, borough_counts_90], axis=1, keys=['Total_listings', 'More_than_90'])\n",
        "\n",
        "# Calculate the ratio of listings with more than 90 booked nights per total listings\n",
        "combined_data['Ratio_of_more_than_90'] = combined_data['More_than_90'] / combined_data['Total_listings']\n",
        "\n",
        "# Fill any NaN values that might occur if there are boroughs with no listings > 90 nights\n",
        "combined_data['Ratio_of_more_than_90'] = combined_data['Ratio_of_more_than_90'].fillna(0)\n",
        "\n",
        "# Data formatting and round to four decimal places\n",
        "combined_data['Ratio_of_more_than_90'] = combined_data['Ratio_of_more_than_90'].apply(lambda x: round(x, 4))\n",
        "\n",
        "# Rename the index label to 'Borough_name'\n",
        "combined_data.index.rename('Borough_name', inplace=True)\n",
        "\n",
        "# Load the borough codes\n",
        "borough_code_file_path = 'data/borough_name_code.csv'\n",
        "borough_codes = pd.read_csv(borough_code_file_path)\n",
        "\n",
        "# Reset index in combined_data to turn the index into a regular column\n",
        "combined_data.reset_index(inplace=True)\n",
        "borough_codes.reset_index(inplace=True)\n",
        "\n",
        "#Combine the ratio data and borough name with borough code by borough name\n",
        "combined_data = pd.merge(combined_data, borough_codes[['Borough_name', 'Borough_code']], on='Borough_name', how='left')\n",
        "\n",
        "# Set 'Borough_name' back as the index\n",
        "combined_data.set_index('Borough_name', inplace=True)\n",
        "\n",
        "# Save the updated DataFrame\n",
        "combined_data.to_csv('data/borough_listings_ratio.csv', index=True)"
      ],
      "id": "0c814d98",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cache": true
      },
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "# Moran analysis\n",
        "from esda.moran import Moran  \n",
        "# Load data\n",
        "ratio = pd.read_csv(\"data/borough_listings_ratio.csv\")\n",
        "borough = gpd.read_file(\"data/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp\")\n",
        "\n",
        "# merge\n",
        "borough_ratio = borough.merge(ratio, left_on=\"GSS_CODE\", right_on=\"Borough_code\")\n",
        "\n",
        "# Calculate neighbors using Queen contiguity\n",
        "weights = Queen.from_dataframe(borough_ratio)\n",
        "weights.transform = 'r'  # Row-standardize the weights\n",
        "\n",
        "os.makedirs('plots/raw', exist_ok=True)\n",
        "\n",
        "# Global Moran's I\n",
        "y = borough_ratio['Ratio_of_more_than_90']\n",
        "moran = Moran(y, weights)\n",
        "print(f\"Global Moran's I: {moran.I:.3f}\")\n",
        "print(f\"P-value: {moran.p_sim:.3f}\")\n",
        "\n",
        "# Moran Plot\n",
        "def moran_plot(y, weights):\n",
        "    lag = weights.sparse.dot(y)\n",
        "    slope, intercept = np.polyfit(y, lag, 1)\n",
        "    \n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.scatter(y, lag)\n",
        "    plt.plot(y, slope * y + intercept, 'r')\n",
        "    plt.xlabel('Ratio of more than 90')\n",
        "    plt.ylabel('Spatially Lagged Ratio')\n",
        "    plt.title(\"Moran Plot of rule breaking Airbnbs\")\n",
        "    plt.axvline(y.mean(), color='gray', linestyle='--')\n",
        "    plt.axhline(lag.mean(), color='gray', linestyle='--')\n",
        "    plt.savefig('plots/raw/Moran_rule_breaking.png') \n",
        "    plt.show()\n",
        "\n",
        "moran_plot(y, weights)"
      ],
      "id": "16e6e042",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cache": true
      },
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "# Local Moran's I\n",
        "from esda.moran import Moran_Local \n",
        "local_moran = Moran_Local(y, weights)\n",
        "borough_ratio['Ii'] = local_moran.Is\n",
        "borough_ratio['p_value'] = local_moran.p_sim\n",
        "\n",
        "# Plot Local Moran's I\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "borough_ratio.plot(column='Ii', legend=True, ax=ax)\n",
        "plt.title(\"Local Moran's I Statistics\")\n",
        "plt.axis('off')\n",
        "plt.show()\n",
        "\n",
        "# LISA Cluster Map\n",
        "sig = 0.1\n",
        "labels = ['Not Significant', 'Low-Low', 'Low-High', 'High-Low', 'High-High']\n",
        "colors = ['white', 'blue', 'lightblue', 'pink', 'red']\n",
        "\n",
        "# Standardize the variable of interest\n",
        "y_std = (y - y.mean()) / y.std()\n",
        "lag_std = weights.sparse.dot(y_std)\n",
        "\n",
        "# Create significance masks\n",
        "sig_mask = local_moran.p_sim < sig\n",
        "\n",
        "# Create cluster categories\n",
        "borough_ratio['quadrant'] = np.zeros(len(y))\n",
        "borough_ratio.loc[sig_mask, 'quadrant'] = np.where(y_std < 0,\n",
        "    np.where(lag_std < 0, 1, 2),\n",
        "    np.where(lag_std < 0, 3, 4))[sig_mask]\n",
        "\n",
        "# Plot LISA clusters\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "borough_ratio.plot(column='quadrant', categorical=True, k=5, cmap='Paired',\n",
        "                  legend=True, ax=ax)\n",
        "plt.title('LISA Cluster Map of rule breaking Airbnbs')\n",
        "plt.axis('off')\n",
        "plt.savefig('plots/raw/LISA_rule_breaking.png') \n",
        "plt.show()\n",
        "\n",
        "# Additional analysis plots\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(y, bins=20)\n",
        "plt.title('Distribution of Ratio_of_more_than_90')\n",
        "plt.xlabel('Value')\n",
        "plt.show()\n",
        "\n",
        "print(y.describe())\n",
        "# print(local_moran.Is.describe())\n",
        "print(pd.Series(local_moran.Is).describe())\n",
        "\n",
        "print(f\"Number of significant clusters: {(local_moran.p_sim < 0.1).sum()}\")"
      ],
      "id": "7a5ad34b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "# Distance-based weights (20km)\n",
        "from libpysal.weights import KNN  \n",
        "centroids = borough_ratio.geometry.centroid\n",
        "coords = np.column_stack((centroids.x, centroids.y))\n",
        "knn = KNN.from_dataframe(borough_ratio, k=4)  # Approximate 20km neighbors\n",
        "knn.transform = 'r'\n",
        "\n",
        "# Calculate Local Moran's I with distance weights\n",
        "local_moran_dist = Moran_Local(y, knn)\n",
        "\n",
        "# Add results to GeoDataFrame\n",
        "borough_ratio['Ii_dist'] = local_moran_dist.Is\n",
        "\n",
        "# Plot results with distance-based weights\n",
        "fig, ax = plt.subplots(figsize=(12, 8))\n",
        "borough_ratio.plot(column='Ii_dist', legend=True, ax=ax)\n",
        "plt.title(\"Local Moran Statistic (Distance-based)\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "id": "df0a87b6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cache": true
      },
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "from libpysal.weights import Queen, lag_spatial\n",
        "from esda.moran import Moran_BV, Moran_Local_BV\n",
        "\n",
        "# load data\n",
        "connect = pd.read_csv(\"data/connect.csv\")\n",
        "borough = gpd.read_file(\"data/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp\")\n",
        "\n",
        "# merge the data\n",
        "borough_connect = borough.merge(connect, left_on=\"GSS_CODE\", right_on=\"Borough_code\")"
      ],
      "id": "26ff2753",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cache": true
      },
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "# analyse the spatial autocorrelation of monthly rent and airbnbs breaking the rule\n",
        "# Variables\n",
        "var1 = 'Monthly_rent_2023'\n",
        "var2 = 'Ratio_of_more_than_90'\n",
        "\n",
        "# Check for and handle missing data\n",
        "borough_connect.dropna(subset=[var1, var2], inplace=True)\n",
        "\n",
        "# Create weights and row-standardize them\n",
        "weights = Queen.from_dataframe(borough_connect, use_index=True)\n",
        "weights.transform = 'r'\n",
        "\n",
        "# Bivariate Moran's I\n",
        "moran_bv = Moran_BV(borough_connect[var1], borough_connect[var2], weights)\n",
        "print(f\"Bivariate Moran's I between {var1} and {var2}: {moran_bv.I:.3f}\")\n",
        "print(f\"p-value: {moran_bv.p_sim:.3f}\")\n",
        "\n",
        "# Bivariate Moran Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "spatial_lag_var2 = lag_spatial(weights, borough_connect[var2])  # Calculate the spatial lag of var2\n",
        "scatter = ax.scatter(borough_connect[var1], spatial_lag_var2, color='blue', edgecolor='k', alpha=0.7)\n",
        "fit = np.polyfit(borough_connect[var1], spatial_lag_var2, 1)\n",
        "ax.plot(borough_connect[var1], np.polyval(fit, borough_connect[var1]), color='red', linestyle='--', linewidth=1)\n",
        "ax.set_title('Bivariate Moran Scatter Plot monthly rent and rule breaking Airbnbs')\n",
        "ax.set_xlabel(var1)\n",
        "ax.set_ylabel(f\"Spatial Lag of {var2}\")\n",
        "plt.savefig('plots/raw/Moran_monthly_rent.png')\n",
        "plt.show()\n",
        "\n",
        "# Bivariate Local Moran's I\n",
        "local_moran_bv = Moran_Local_BV(borough_connect[var1], borough_connect[var2], weights)\n",
        "\n",
        "# LISA Plot (Bivariate)\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "borough_connect.assign(cl=local_moran_bv.q).plot(column='cl', categorical=True, \n",
        "                                                 cmap='Paired', linewidth=0.1, ax=ax, \n",
        "                                                 edgecolor='white', legend=True)\n",
        "labels = ['Not Significant', 'Low-Low', 'Low-High', 'High-Low', 'High-High']\n",
        "legend = ax.get_legend()\n",
        "if legend:\n",
        "    legend.set_bbox_to_anchor((1, 1))\n",
        "    legend.set_title('Cluster Type')\n",
        "    for text, label in zip(legend.get_texts(), labels):\n",
        "        text.set_text(label)\n",
        "\n",
        "ax.set_title('Bivariate LISA Cluster Map of monthly rent and rule breaking Airbnbs')\n",
        "ax.set_axis_off()\n",
        "plt.savefig('plots/raw/LISA_monthly_rent.png')\n",
        "plt.show()"
      ],
      "id": "08fa7956",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cache": true
      },
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "# analyse the spatial autocorrelation of vacant ratio and airbnbs breaking the rule\n",
        "# Variables\n",
        "var1 = 'Vacant_Ratio'\n",
        "var2 = 'Ratio_of_more_than_90'\n",
        "\n",
        "# Check for and handle missing data\n",
        "borough_connect.dropna(subset=[var1, var2], inplace=True)\n",
        "\n",
        "# Create weights and row-standardize them\n",
        "weights = Queen.from_dataframe(borough_connect, use_index=True)\n",
        "weights.transform = 'r'\n",
        "\n",
        "# Bivariate Moran's I\n",
        "moran_bv = Moran_BV(borough_connect[var1], borough_connect[var2], weights)\n",
        "print(f\"Bivariate Moran's I between {var1} and {var2}: {moran_bv.I:.3f}\")\n",
        "print(f\"p-value: {moran_bv.p_sim:.3f}\")\n",
        "\n",
        "# Bivariate Moran Plot\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "spatial_lag_var2 = lag_spatial(weights, borough_connect[var2])  # Calculate the spatial lag of var2\n",
        "scatter = ax.scatter(borough_connect[var1], spatial_lag_var2, color='blue', edgecolor='k', alpha=0.7)\n",
        "fit = np.polyfit(borough_connect[var1], spatial_lag_var2, 1)\n",
        "ax.plot(borough_connect[var1], np.polyval(fit, borough_connect[var1]), color='red', linestyle='--', linewidth=1)\n",
        "ax.set_title('Bivariate Moran Scatter Plot of vacant ratio and rule breaking Airbnbs')\n",
        "ax.set_xlabel(var1)\n",
        "ax.set_ylabel(f\"Spatial Lag of {var2}\")\n",
        "plt.savefig('plots/raw/Moran_vacant_ratio.png')\n",
        "plt.show()\n",
        "\n",
        "# Bivariate Local Moran's I\n",
        "local_moran_bv = Moran_Local_BV(borough_connect[var1], borough_connect[var2], weights)\n",
        "\n",
        "# LISA Plot (Bivariate)\n",
        "fig, ax = plt.subplots(figsize=(10, 10))\n",
        "borough_connect.assign(cl=local_moran_bv.q).plot(column='cl', categorical=True, \n",
        "                                                 cmap='Paired', linewidth=0.1, ax=ax, \n",
        "                                                 edgecolor='white', legend=True)\n",
        "labels = ['Not Significant', 'Low-Low', 'Low-High', 'High-Low', 'High-High']\n",
        "legend = ax.get_legend()\n",
        "if legend:\n",
        "    legend.set_bbox_to_anchor((1, 1))\n",
        "    legend.set_title('Cluster Type')\n",
        "    for text, label in zip(legend.get_texts(), labels):\n",
        "        text.set_text(label)\n",
        "\n",
        "ax.set_title('Bivariate LISA Cluster Map of vacant ratio and rule breaking Airbnbs')\n",
        "ax.set_axis_off()\n",
        "plt.savefig('plots/raw/LISA_vacant_ratio.png')\n",
        "plt.show()"
      ],
      "id": "e39a149a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "# Plotting the combined figure showing the rusults of Moran scatter plot and LISA cluster map\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# Paths to the images\n",
        "morans = ['plots/raw/Moran_rule_breaking.png', 'plots/raw/Moran_monthly_rent.png', 'plots/raw/Moran_vacant_ratio.png']\n",
        "lisas = ['plots/raw/LISA_rule_breaking.png', 'plots/raw/LISA_monthly_rent.png', 'plots/raw/LISA_vacant_ratio.png']\n",
        "\n",
        "# Load all images\n",
        "images = [Image.open(img) for img in morans + lisas]\n",
        "\n",
        "# Calculate total width and height for the new image\n",
        "total_width = images[0].width * 3\n",
        "max_height = images[0].height + images[3].height \n",
        "\n",
        "# Create a new image with the appropriate size\n",
        "new_im = Image.new('RGB', (total_width, max_height))\n",
        "\n",
        "# Paste each Moran plot into the new image\n",
        "for i, img in enumerate(images[:3]):  # First three are Moran plots\n",
        "    new_im.paste(img, (img.width * i, 0))\n",
        "\n",
        "# Paste each LISA plot into the new image\n",
        "for i, img in enumerate(images[3:]):  # Last three are LISA plots\n",
        "    new_im.paste(img, (img.width * i, images[0].height))  # Paste below the Moran plots\n",
        "\n",
        "new_im.save('plots/combined_of_Moran_and_LISA.png')"
      ],
      "id": "1dc51d88",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt  \n",
        "import matplotlib.image as mpimg  \n",
        "\n",
        "# Create Layout  \n",
        "fig = plt.figure(figsize=(16, 5.5), dpi=800)  \n",
        "\n",
        "# Path of png  \n",
        "img = mpimg.imread('plots/combined_of_Moran_and_LISA.png')  \n",
        "\n",
        "# Adjust layout \n",
        "ax = fig.add_axes([0.05, 0.1, 0.9, 0.85])  \n",
        "ax.imshow(img)  \n",
        "ax.axis('off')  \n",
        "# title position\n",
        "fig.text(0.5, 0.015, 'Figure 3: Results of Moran and LISA analysis of rule breaking Airbnbs,\\nmonthly rent and vacancy ratio',  \n",
        "         ha='center', fontsize=14, fontweight='bold')  \n",
        " \n",
        "plt.show()"
      ],
      "id": "dbcce01e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. **Housing Market Impacts**  \n",
        "To quantify these impacts, we applied SAR and GWR models (Figure 4). The SAR analysis showed that violations contributed to rising rents and increased vacancy rates, with the strongest effects observed in central areas where tourism dominates and in eastern boroughs with emerging rental markets. GWR results highlighted spatial variability, aligning with findings from [@jain2021nowcasting], the highest rent surges in central London and higher vacancy rates in eastern boroughs.   \n"
      ],
      "id": "c1ff3ce8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "# SAR model\n",
        "from spreg import ML_Lag\n",
        "# Import data\n",
        "data = pd.read_csv(\"data/connect.csv\")\n",
        "shp = gpd.read_file(\"data/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp\")\n",
        "\n",
        "# Merge data and transform coordinate system\n",
        "zone = shp.merge(data, left_on=\"GSS_CODE\", right_on=\"Borough_code\")\n",
        "zone = zone.to_crs(\"EPSG:27700\")\n",
        "\n",
        "# Check and remove missing values\n",
        "columns = ['Monthly_rent_2023', 'Vacant_Ratio', 'Ratio_of_more_than_90']\n",
        "print(\"Missing values:\\n\", zone[columns].isna().sum())\n",
        "zone = zone.dropna(subset=columns)\n",
        "\n",
        "# Construct spatial weights matrix\n",
        "w = Queen.from_dataframe(zone)\n",
        "w.transform = 'r'\n",
        "\n",
        "# Prepare variables\n",
        "y = zone['Ratio_of_more_than_90'].values.reshape(-1, 1)\n",
        "X = zone[['Monthly_rent_2023', 'Vacant_Ratio']].values\n",
        "\n",
        "# Fit Spatial Lag Model\n",
        "sar_model = ML_Lag(y, X, w=w,\n",
        "                   name_y='Ratio_of_more_than_90',\n",
        "                   name_x=['Monthly_rent_2023', 'Vacant_Ratio'],\n",
        "                   name_w='w')\n",
        "\n",
        "# Output model results\n",
        "print(\"=== SAR Model Results ===\")\n",
        "print(sar_model.summary)\n",
        "\n",
        "# Visualize residuals\n",
        "zone['residuals'] = sar_model.u\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "zone.plot(column='residuals', cmap='viridis', legend=True, ax=ax)\n",
        "plt.title(\"SAR Model Residuals\")\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "id": "9418448d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cache": true
      },
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "#| include: false\n",
        "# GWR model\n",
        "from mgwr.gwr import GWR\n",
        "from mgwr.sel_bw import Sel_BW\n",
        "zone = zone.to_crs(\"EPSG:27700\")\n",
        "zone['centro'] = zone.geometry.centroid\n",
        "zone['X'] = zone['centro'].x\n",
        "zone['Y'] = zone['centro'].y\n",
        "g_y_rent = zone['Monthly_rent_2023'].values.reshape((-1, 1))\n",
        "g_X_rent = zone[['Ratio_of_more_than_90']].values\n",
        "g_coords = list(zip(zone['X'], zone['Y']))\n",
        "\n",
        "# Automatically set bw_min and bw_max based on the number of observations\n",
        "n_obs = len(g_coords)  # Number of observations\n",
        "bw_min = 2  # Minimum bandwidth, should be a positive integer\n",
        "bw_max = max(bw_min, n_obs - 1)  # Ensures bw_max does not exceed n_obs - 1\n",
        "\n",
        "# Initialize bandwidth selector with dynamic bandwidth settings\n",
        "gwr_selector_rent = Sel_BW(g_coords, g_y_rent, g_X_rent, fixed=False)\n",
        "\n",
        "# Search for optimal bandwidth using the golden section search method\n",
        "gwr_bw_rent = gwr_selector_rent.search(search_method='golden_section', criterion='AICc', bw_min=bw_min, bw_max=bw_max)\n",
        "print('Optimal Bandwidth Size for Rent:', gwr_bw_rent)\n",
        "\n",
        "# Fit GWR model with the determined optimal bandwidth\n",
        "gwr_results_rent = GWR(g_coords, g_y_rent, g_X_rent, gwr_bw_rent, fixed=False, kernel='bisquare').fit()"
      ],
      "id": "566c1bfc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "#| include: false\n",
        "g_coords = list(zip(zone['X'], zone['Y']))\n",
        "\n",
        "# Define independent and dependent variables for the Vacant_Ratio model\n",
        "g_y_vacant = zone['Vacant_Ratio'].values.reshape((-1, 1))\n",
        "g_X_vacant = zone[['Ratio_of_more_than_90']].values\n",
        "\n",
        "# Automatically set bw_min and bw_max based on the number of observations\n",
        "n_obs = len(g_coords)  # Number of observations\n",
        "bw_min = 2  # Minimum bandwidth, should be a positive integer\n",
        "bw_max = max(bw_min, n_obs - 1)  # Ensures bw_max does not exceed n_obs - 1\n",
        "\n",
        "# Initialize bandwidth selector with dynamic bandwidth settings for Vacant_Ratio\n",
        "gwr_selector_vacant = Sel_BW(g_coords, g_y_vacant, g_X_vacant, fixed=False)\n",
        "\n",
        "# Search for optimal bandwidth using the golden section search method for Vacant_Ratio\n",
        "gwr_bw_vacant = gwr_selector_vacant.search(search_method='golden_section', criterion='AICc', bw_min=bw_min, bw_max=bw_max)\n",
        "print('Optimal Bandwidth Size for Vacant Ratio:', gwr_bw_vacant)\n",
        "\n",
        "# Fit GWR model with the determined optimal bandwidth for Vacant_Ratio\n",
        "gwr_results_vacant = GWR(g_coords, g_y_vacant, g_X_vacant, gwr_bw_vacant, fixed=False, kernel='bisquare').fit()\n",
        "print(gwr_results_vacant.summary())"
      ],
      "id": "41d4dab6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "zone['coefficient'] = gwr_results_rent.params[:, 1]  # Add coefficients\n",
        "zone['t_values'] = gwr_results_rent.tvalues[:, 1]  # Add t-values"
      ],
      "id": "adeb5dfa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "#| include: false\n",
        "# Define the variable names to be visualized, corresponding to the regression results added\n",
        "var_names = ['coefficient']  # Adjust this if more variables from the model should be visualized\n",
        "\n",
        "fig, axes = plt.subplots(1, len(var_names), figsize=(12, 3))\n",
        "\n",
        "# Ensure `axes` is iterable\n",
        "if len(var_names) == 1:\n",
        "    axes = [axes]\n",
        "\n",
        "for i, var in enumerate(var_names):\n",
        "    ax = axes[i]  # Access each subplot axis\n",
        "    zone.plot(column=var, cmap='viridis', legend=True, ax=ax, edgecolor='white', legend_kwds={'label': \"Coefficient value\"})\n",
        "    ax.set_title(f'Regression Coefficients for {var}')\n",
        "    ax.set_axis_off()\n",
        "\n",
        "    # Highlight non-significant areas based on a significance threshold\n",
        "    threshold = 1.96\n",
        "    non_significant = zone['t_values'].abs() < threshold  # Ensuring the use of absolute value for significance checking\n",
        "    zone.loc[non_significant].plot(ax=ax, color='lightgrey', edgecolor='white')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "938a6f7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "# Fit GWR for Monthly_rent_2023\n",
        "gwr_model_rent = GWR(g_coords, zone['Monthly_rent_2023'].values.reshape((-1, 1)),\n",
        "                     zone[['Ratio_of_more_than_90']].values.reshape((-1, 1)), gwr_bw_rent).fit()\n",
        "\n",
        "# Fit GWR for Vacant_Ratio\n",
        "gwr_model_vacant = GWR(g_coords, zone['Vacant_Ratio'].values.reshape((-1, 1)),\n",
        "                       zone[['Ratio_of_more_than_90']].values.reshape((-1, 1)), gwr_bw_vacant).fit()\n",
        "\n",
        "# Extract coefficients and t-values for each model\n",
        "rent_coefs = pd.DataFrame(gwr_model_rent.params, columns=['Intercept', 'Effect_of_Ratio_of_more_than_90_on_Rent'])\n",
        "rent_tvals = pd.DataFrame(gwr_model_rent.tvalues, columns=['t_Intercept', 't_Effect_on_Rent'])\n",
        "\n",
        "vacant_coefs = pd.DataFrame(gwr_model_vacant.params, columns=['Intercept', 'Effect_of_Ratio_of_more_than_90_on_Vacancy'])\n",
        "vacant_tvals = pd.DataFrame(gwr_model_vacant.tvalues, columns=['t_Intercept', 't_Effect_on_Vacancy'])"
      ],
      "id": "3cf045a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "# Add results directly to zone GeoDataFrame\n",
        "zone['Rent_Effect'] = rent_coefs['Effect_of_Ratio_of_more_than_90_on_Rent']\n",
        "zone['Vacancy_Effect'] = vacant_coefs['Effect_of_Ratio_of_more_than_90_on_Vacancy']\n",
        "\n",
        "# Check significance and add to zone\n",
        "zone['Significant_Rent'] = rent_tvals['t_Effect_on_Rent'].abs() > 1.96\n",
        "zone['Significant_Vacancy'] = vacant_tvals['t_Effect_on_Vacancy'].abs() > 1.96"
      ],
      "id": "82828607",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "#| include: false\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
        "\n",
        "# Plot for Rent\n",
        "zone.plot(column='Rent_Effect', cmap='viridis', ax=ax[0], legend=True,\n",
        "          legend_kwds={'label': \"Effect on Rent\"})\n",
        "zone[~zone['Significant_Rent']].plot(color='lightgrey', ax=ax[0])\n",
        "ax[0].set_title('Effect of Ratio_of_more_than_90 on Rent')\n",
        "ax[0].set_axis_off()\n",
        "\n",
        "# Plot for Vacancy\n",
        "zone.plot(column='Vacancy_Effect', cmap='viridis', ax=ax[1], legend=True,\n",
        "          legend_kwds={'label': \"Effect on Vacancy\"})\n",
        "zone[~zone['Significant_Vacancy']].plot(color='lightgrey', ax=ax[1])\n",
        "ax[1].set_title('Effect of Ratio_of_more_than_90 on Vacancy')\n",
        "ax[1].set_axis_off()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "59f82396",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cache": true
      },
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "#| include: false\n",
        "# combing the plots to a new plot\n",
        "zone['residuals'] = sar_model.u\n",
        "\n",
        "# Create a figure with three subplots (one row, three columns)\n",
        "fig, ax = plt.subplots(1, 3, figsize=(18, 6))  # Adjust the figure size as needed\n",
        "\n",
        "# Plot for Residuals\n",
        "zone.plot(column='residuals', cmap='viridis', ax=ax[0], legend=True)\n",
        "ax[0].set_title('SAR Model Residuals')\n",
        "ax[0].set_axis_off()\n",
        "\n",
        "# Plot for Rent Effect\n",
        "zone.plot(column='Rent_Effect', cmap='viridis', ax=ax[1], legend=True, legend_kwds={'label': \"Effect on Rent\"})\n",
        "zone[~zone['Significant_Rent']].plot(color='lightgrey', ax=ax[1])\n",
        "ax[1].set_title('Effect of rule breaking Airbnbs on rent')\n",
        "ax[1].set_axis_off()\n",
        "\n",
        "# Plot for Vacancy Effect\n",
        "zone.plot(column='Vacancy_Effect', cmap='viridis', ax=ax[2], legend=True, legend_kwds={'label': \"Effect on Vacancy\"})\n",
        "zone[~zone['Significant_Vacancy']].plot(color='lightgrey', ax=ax[2])\n",
        "ax[2].set_title('Effect of rule breaking Airbnbs on vacancy')\n",
        "ax[2].set_axis_off()\n",
        "\n",
        "#output\n",
        "plt.savefig('plots/Results_of_SAR_and_GWR_model.png', dpi=300, bbox_inches='tight')  \n",
        "\n",
        "# Adjust layout\n",
        "plt.tight_layout()"
      ],
      "id": "d0788466",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import matplotlib.pyplot as plt  \n",
        "import matplotlib.image as mpimg  \n",
        "\n",
        "# Create Layout\n",
        "fig = plt.figure(figsize=(16, 6))  # 减小高度更紧凑  \n",
        "\n",
        "# Path of png \n",
        "img = mpimg.imread('plots/Results_of_SAR_and_GWR_model.png')  # 替换为你的图片路径  \n",
        "\n",
        "# \n",
        "ax = fig.add_axes([0.05, 0.18, 0.9, 0.75])  # [左, 下, 宽, 高]，上下的空隙进一步减少  \n",
        "ax.imshow(img)  \n",
        "ax.axis('off')  # 隐藏坐标轴  \n",
        "\n",
        "# Title\n",
        "fig.text(0.5, 0, 'Figure 4: Results of SAR and GWR Analysis of the Effect of Rule-Breaking Airbnbs\\non Monthly Rent and Vacancy Ratio',   \n",
        "         ha='center', fontsize=14, fontweight='bold')  # 标题紧贴底部，换行确保清晰  \n",
        "\n",
        "# \n",
        "plt.show()"
      ],
      "id": "320e9111",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From the analysis above, enforcing the 90-day policy is essential to address rising rents, increasing vacancy rates, and spatial inequality driven by short-term rentals, thereby preserving housing affordability and stability in affected hotspots.\n",
        "\n",
        "---\n",
        "\n",
        "### Analysis of Factors Associated with Policy-Violating Listings\n",
        "\n",
        "1. **Logistic Regression:** To explore the relationship in the study area between variables and their spatial distribution characteristics, we adopted a logistic regression model to establish the model and evaluated the performance of the model through the residual and spatial autocorrelation test.\n"
      ],
      "id": "9fa99df4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "# Imports  \n",
        "import pandas as pd  \n",
        "import numpy as np  \n",
        "import matplotlib.pyplot as plt  \n",
        "import seaborn as sns  \n",
        "from scipy import stats  \n",
        "from statsmodels.tools import add_constant  \n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor  \n",
        "import statsmodels.api as sm  \n",
        "from shapely.geometry import Point  \n",
        "from libpysal.weights import KNN  \n",
        "from esda.moran import Moran  \n",
        "from splot.esda import moran_scatterplot  \n",
        "import geopandas as gpd  \n",
        "from pandas.plotting import table  \n",
        "import matplotlib.gridspec as gridspec  "
      ],
      "id": "d9f88bb7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "# Load and preprocess data   \n",
        "regression_data = airbnb_data\n",
        "\n",
        "# Select and clean columns  \n",
        "columns_to_keep = [  \n",
        "    \"estimated_nights_booked\",   \n",
        "    \"room_type\",   \n",
        "    \"price\",   \n",
        "    \"minimum_nights\",  \n",
        "    \"calculated_host_listings_count\",   \n",
        "    \"longitude\",   \n",
        "    \"latitude\"  \n",
        "]  \n",
        "regression_data = regression_data[columns_to_keep].dropna()  \n",
        "\n",
        "# Create binary outcome for nights booked  \n",
        "threshold = regression_data[\"estimated_nights_booked\"].median()  \n",
        "regression_data[\"estimated_nights_booked_binary\"] = (  \n",
        "    regression_data[\"estimated_nights_booked\"] > threshold  \n",
        ").astype(int)  \n",
        "\n",
        "# Encode 'room_type' using OneHotEncoder  \n",
        "from sklearn.preprocessing import OneHotEncoder  \n",
        "encoder = OneHotEncoder(drop=\"first\", sparse_output=False)  \n",
        "room_type_encoded = encoder.fit_transform(regression_data[[\"room_type\"]])  \n",
        "room_type_columns = encoder.get_feature_names_out([\"room_type\"])  \n",
        "room_type_df = pd.DataFrame(room_type_encoded, columns=room_type_columns, index=regression_data.index)  \n",
        "\n",
        "# Add multi-list host column  \n",
        "regression_data[\"multi_list_host\"] = (  \n",
        "    regression_data[\"calculated_host_listings_count\"] > 1  \n",
        ").astype(int)  \n",
        "\n",
        "# Drop columns no longer needed and combine encoded features  \n",
        "regression_data = regression_data.drop(columns=[\"calculated_host_listings_count\", \"room_type\"])  \n",
        "regression_data = pd.concat([regression_data, room_type_df], axis=1)  "
      ],
      "id": "7548d5b0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "#| include: false\n",
        "\n",
        "# VIF Calculation  \n",
        "X = regression_data.drop(  \n",
        "    columns=[\"estimated_nights_booked\", \"estimated_nights_booked_binary\", \"longitude\", \"latitude\"]  \n",
        ")  \n",
        "X = add_constant(X)  \n",
        "\n",
        "# Calculate VIF  \n",
        "vif_data = pd.DataFrame()  \n",
        "vif_data[\"Variable\"] = X.columns  \n",
        "vif_data[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]  \n",
        "\n",
        "vif_results = vif_data  "
      ],
      "id": "04472eee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "#| include: false\n",
        "# Run Logistic Regression  \n",
        "Y = regression_data[\"estimated_nights_booked_binary\"]  \n",
        "logit_model = sm.Logit(Y, X)  \n",
        "result = logit_model.fit()  \n",
        "\n",
        "# Extract and display summary table  \n",
        "regression_summary = result.summary().tables[1]  \n",
        "data = regression_summary.data[1:]  # Skip the header row  \n",
        "headers = regression_summary.data[0]  # Use the header  \n",
        "summary_df = pd.DataFrame(data, columns=headers)  \n",
        "\n",
        "# Add predicted probabilities and residuals  \n",
        "regression_data[\"predicted_probs\"] = result.predict(X)  \n",
        "regression_data[\"residuals\"] = (  \n",
        "    regression_data[\"estimated_nights_booked_binary\"] - regression_data[\"predicted_probs\"]  \n",
        ")  \n",
        "\n",
        "print(result.summary())  \n",
        "print(\"Logistic regression complete.\")"
      ],
      "id": "e57dc897",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| output: false\n",
        "#| include: false\n",
        "\n",
        "# Clip predicted probabilities  \n",
        "regression_data[\"predicted_probs\"] = regression_data[\"predicted_probs\"].clip(1e-6, 1 - 1e-6)  \n",
        "\n",
        "# Calculate Deviance Residuals  \n",
        "eps = 1e-6  # Small constant to avoid numerical issues  \n",
        "regression_data[\"deviance_residuals\"] = np.sign(  \n",
        "    regression_data[\"estimated_nights_booked_binary\"] - regression_data[\"predicted_probs\"]  \n",
        ") * np.sqrt(  \n",
        "    -2 * (  \n",
        "        regression_data[\"estimated_nights_booked_binary\"] * np.log(regression_data[\"predicted_probs\"] + eps) +  \n",
        "        (1 - regression_data[\"estimated_nights_booked_binary\"]) * np.log(1 - regression_data[\"predicted_probs\"] + eps)  \n",
        "    )  \n",
        ")  \n",
        "\n",
        "# Convert data into GeoDataFrame  \n",
        "regression_data[\"geometry\"] = [  \n",
        "    Point(xy) for xy in zip(regression_data[\"longitude\"], regression_data[\"latitude\"])  \n",
        "]  \n",
        "gdf = gpd.GeoDataFrame(regression_data, geometry=\"geometry\", crs=\"EPSG:4326\")  \n",
        "\n",
        "# Create KNN weight matrix  \n",
        "weights = KNN.from_dataframe(gdf, k=3)  # k=3 means each point will connect to its 3 nearest neighbors  \n",
        "\n",
        "# Check for isolated points in the weight matrix  \n",
        "islands = weights.islands  # Return a list of indices that are isolated/disconnected  \n",
        "\n",
        "if len(islands) > 0:  \n",
        "    print(f\"Found {len(islands)} isolated points. Removing and recalculating weights...\")  \n",
        "    gdf_filtered = gdf.drop(islands, axis=0)  # Drop the rows corresponding to isolated points  \n",
        "    weights = KNN.from_dataframe(gdf_filtered, k=3)  # Recalculate weights for the filtered dataset  \n",
        "else:  \n",
        "    print(\"No isolated points found. Continuing with the original dataset.\")  \n",
        "    gdf_filtered = gdf  # If no isolated points, the original dataset is used  \n",
        "\n",
        "# Calculate Moran's I  \n",
        "moran = Moran(gdf_filtered[\"deviance_residuals\"], weights)  \n",
        "\n",
        "# Print Moran's I results  \n",
        "print(\"Moran's I Results:\")  \n",
        "print(f\"Moran's I: {moran.I:.4f}\")  \n",
        "print(f\"p-value: {moran.p_sim:.4f}\")  \n",
        "print(f\"z-score: {moran.z_sim:.4f}\")  "
      ],
      "id": "b21f087d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "# Create a 3x2 subplot layout  \n",
        "fig = plt.figure(figsize=(15, 18))  \n",
        "\n",
        "# Residuals vs Fitted Values  \n",
        "ax1 = plt.subplot(3, 2, 1)  \n",
        "sns.scatterplot(x=gdf_filtered[\"predicted_probs\"], y=gdf_filtered[\"deviance_residuals\"],  \n",
        "                ax=ax1, color=\"blue\", s=25)  \n",
        "ax1.axhline(y=0, color=\"red\", linestyle=\"--\", linewidth=1)  \n",
        "ax1.set_title(\"Residuals vs Fitted (Deviance Residuals)\")  \n",
        "ax1.set_xlabel(\"Fitted values / Predicted Probabilities\")  \n",
        "ax1.set_ylabel(\"Deviance Residuals\")  \n",
        "\n",
        "# Q-Q Plot  \n",
        "ax2 = plt.subplot(3, 2, 2)  \n",
        "sm.qqplot(gdf_filtered[\"deviance_residuals\"], line=\"45\", ax=ax2)  \n",
        "ax2.set_title(\"Q-Q Plot (Deviance Residuals)\")  \n",
        "\n",
        "# Scale-Location Plot  \n",
        "ax3 = plt.subplot(3, 2, 3)  \n",
        "sns.scatterplot(x=gdf_filtered[\"predicted_probs\"],   \n",
        "                y=np.sqrt(np.abs(gdf_filtered[\"deviance_residuals\"])),  \n",
        "                ax=ax3, color=\"blue\", s=25)  \n",
        "ax3.axhline(y=0, color=\"red\", linestyle=\"--\", linewidth=1)  \n",
        "ax3.set_title(\"Scale-Location (√|Deviance Residuals|)\")  \n",
        "ax3.set_xlabel(\"Fitted values / Predicted Probabilities\")  \n",
        "ax3.set_ylabel(\"√|Deviance Residuals|\")  \n",
        "\n",
        "# Residuals vs Leverage  \n",
        "ax4 = plt.subplot(3, 2, 4)  \n",
        "try:  \n",
        "    influence = result.get_influence()  \n",
        "    leverage = influence.hat_matrix_diag  \n",
        "    deviance_residuals = gdf_filtered[\"deviance_residuals\"]  \n",
        "    \n",
        "    ax4.scatter(leverage, deviance_residuals ** 2, alpha=0.5, color=\"blue\", s=25)  \n",
        "    ax4.axhline(y=0, color=\"red\", linestyle=\"--\", linewidth=1)  \n",
        "    ax4.set_title(\"Residuals vs Leverage (Deviance Residuals)\")  \n",
        "    ax4.set_xlabel(\"Leverage\")  \n",
        "    ax4.set_ylabel(\"Deviance Residuals^2\")  \n",
        "except Exception as e:  \n",
        "    print(\"Error during Residuals vs Leverage plotting:\", e)  \n",
        "\n",
        "# Moran's I Scatterplot  \n",
        "ax5 = plt.subplot(3, 2, (5, 6))  # Span both columns in the last row  \n",
        "moran_scatterplot(moran, ax=ax5)  \n",
        "ax5.set_title(\"Moran's I Scatterplot (Deviance Residuals)\", fontsize=14)  \n",
        "\n",
        "# 调整布局  \n",
        "plt.tight_layout(rect=[0, 0.05, 1, 1])  # 预留底部空间来放置标题  \n",
        "\n",
        "# 将主标题放置在底部  \n",
        "plt.figtext(0.5, 0.01,  # 设置位置：靠底部 (y=0.01)  \n",
        "            \"Figure 5. Visualisation of Residuals and Moran's I\",   \n",
        "            ha='center',  # 水平居中  \n",
        "            fontsize=16,  # 字体大小  \n",
        "            fontweight='bold')  # 加粗字体  \n",
        "\n",
        "# 显示图形  \n",
        "plt.show()"
      ],
      "id": "b739da20",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Key property and host characteristics were identified as significant drivers of policy violations (p < 0.001). However, the low pseudo R-squared (0.05) suggests the model explains only a small portion of the variability in policy violations.  \n",
        "\n",
        "Residual analysis and Moran's I test (I = 0.0855, p = 0.001) highlighted weak but significant spatial patterns, indicating **missing factors** and **spatial heterogeneity**. Further model improvements are needed to better capture these complexities.\n",
        "\n",
        "---\n",
        "\n",
        "2. **Geographically Weighted Regression:** To address spatial heterogeneity, research suggests adopting the Geographically Weighted Regression (GWR) model, which allows variable coefficients to vary by location. Meanwhile, previous studies have shown a strong association between public transport density, green space density, and tourist attraction density with Airbnb distribution [@xu_influence_2020]. Thus, we incorporate these variables into the model to examine whether they similarly influence the occurrence of non-compliant listings. The data for these variables were obtained from OpenStreetMap [@geofabrik2024london].\n"
      ],
      "id": "1558b07c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| output: false\n",
        "\n",
        "# Step 1: Import Required Libraries\n",
        "import geopandas as gpd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from matplotlib.font_manager import FontProperties\n",
        "from scipy.stats import chi2_contingency, pearsonr, pointbiserialr\n",
        "from shapely.geometry import Point\n",
        "from mgwr.gwr import GWR, MGWR\n",
        "from mgwr.sel_bw import Sel_BW\n",
        "from shapely.geometry import Point"
      ],
      "id": "2cf0bc22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| output: false\n",
        "\n",
        "# Step 2: Load Data\n",
        "## 2.1. Load Airbnb listings data\n",
        "listing = pd.read_csv(\n",
        "    \"data/processed_airbnb_data.csv\"\n",
        ")  # Estimated nights of Airbnb stays\n",
        "\n",
        "## 2.2. Load London ward boundaries and reproject to EPSG:27700\n",
        "ward = gpd.read_file(\n",
        "    \"data/statistical-gis-boundaries-london/ESRI/London_Ward_CityMerged.shp\"\n",
        ").to_crs(epsg=27700)\n",
        "\n",
        "## 2.3. Load points of interest (POIs) and reproject\n",
        "nature = gpd.read_file(\"data/ldn_poi/gis_osm_natural_free_1.shp\").to_crs(\n",
        "    epsg=27700\n",
        ")  # Nature POIs\n",
        "transport = gpd.read_file(\"data/ldn_poi/gis_osm_transport_free_1.shp\").to_crs(\n",
        "    epsg=27700\n",
        ")  # Transport POIs\n",
        "tourism = gpd.read_file(\"data/ldn_poi/gis_osm_pois_free_1.shp\").to_crs(\n",
        "    epsg=27700\n",
        ")  # Tourism POIs"
      ],
      "id": "969ea587",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| output: false\n",
        "\n",
        "# Step 3: Convert Listings Data to GeoDataFrame\n",
        "## 3.1. Convert CSV longitude/latitude data into a GeoDataFrame\n",
        "geometry = [\n",
        "    Point(xy) for xy in zip(listing[\"longitude\"], listing[\"latitude\"])\n",
        "]  # Create Point geometries\n",
        "listing_spatial = gpd.GeoDataFrame(\n",
        "    listing, geometry=geometry, crs=\"EPSG:4326\"\n",
        ")  # Set GeoDataFrame with CRS\n",
        "\n",
        "## 3.2. Reproject to British National Grid (EPSG:27700)\n",
        "listing_spatial = listing_spatial.to_crs(epsg=27700)"
      ],
      "id": "ab6e2463",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| output: false\n",
        "\n",
        "# Step 4: Calculate Density of Listings > 90 Nights per Ward (Y)\n",
        "## 4.1. Filter listings where estimated nights are >= 90\n",
        "listing_spatial_90 = listing_spatial[listing_spatial[\"estimated_nights_booked\"] >= 90]\n",
        "\n",
        "## 4.2. Spatial join: count listings per ward\n",
        "join_listing_ward = gpd.sjoin(\n",
        "    ward, listing_spatial_90, how=\"left\", predicate=\"intersects\"\n",
        ")\n",
        "ward_count = (\n",
        "    join_listing_ward.groupby(\"GSS_CODE\").size().reset_index(name=\"listing_count\")\n",
        ")\n",
        "\n",
        "## 4.3. Merge counts into ward dataset\n",
        "ward = ward.merge(ward_count, on=\"GSS_CODE\", how=\"left\").fillna({\"listing_count\": 0})\n",
        "\n",
        "## 4.4. Calculate area and listing density\n",
        "ward[\"area\"] = ward.geometry.area\n",
        "ward[\"listing_density\"] = ward[\"listing_count\"] / ward[\"area\"]"
      ],
      "id": "090da8a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| output: false\n",
        "\n",
        "# Step 5: Calculate Density of Green Spaces, Transport, and Tourism POIs\n",
        "# --------------------------------------------------------------\n",
        "## 5.1 Density of Green Spaces (X1)\n",
        "### 5.1.1. Spatial join: green spaces per ward\n",
        "join_nature_ward = gpd.sjoin(ward, nature, how=\"left\", predicate=\"intersects\")\n",
        "nature_count = (\n",
        "    join_nature_ward.groupby(\"GSS_CODE\").size().reset_index(name=\"nature_count\")\n",
        ")\n",
        "ward = ward.merge(nature_count, on=\"GSS_CODE\", how=\"left\").fillna({\"nature_count\": 0})\n",
        "\n",
        "### 5.1.2. Calculate green space density\n",
        "ward[\"nature_density\"] = ward[\"nature_count\"] / ward[\"area\"]\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "## 5.2 Density of Transport Networks (X2)\n",
        "### 5.2.1. Spatial join: transport networks per ward\n",
        "join_transport_ward = gpd.sjoin(ward, transport, how=\"left\", predicate=\"intersects\")\n",
        "transport_count = (\n",
        "    join_transport_ward.groupby(\"GSS_CODE\").size().reset_index(name=\"transport_count\")\n",
        ")\n",
        "ward = ward.merge(transport_count, on=\"GSS_CODE\", how=\"left\").fillna(\n",
        "    {\"transport_count\": 0}\n",
        ")\n",
        "\n",
        "### 5.2.2. Calculate transport network density\n",
        "ward[\"transport_density\"] = ward[\"transport_count\"] / ward[\"area\"]\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "## 5.3 Density of Tourism Sites (X3)\n",
        "### 5.3.1 Extract poi related to tourist attractions\n",
        "tourism = tourism[\n",
        "    tourism[\"code\"].astype(str).str.startswith(\"26\")\n",
        "]  # In OSM, places beginning with 26 represent tourist attractions\n",
        "\n",
        "### 5.3.2 Spatial join: tourism sites per ward\n",
        "join_tourism_ward = gpd.sjoin(ward, tourism, how=\"left\", predicate=\"intersects\")\n",
        "ward_count = (\n",
        "    join_tourism_ward.groupby(\"GSS_CODE\").size().reset_index(name=\"tourism_count\")\n",
        ")\n",
        "ward = ward.merge(ward_count, on=\"GSS_CODE\", how=\"left\")\n",
        "ward[\"tourism_count\"] = ward[\"tourism_count\"].fillna(0)\n",
        "\n",
        "### 5.3.3. Calculate tourism site density\n",
        "ward[\"tourism_density\"] = ward[\"tourism_count\"] / ward[\"area\"]"
      ],
      "id": "3a1c8715",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| output: false\n",
        "\n",
        "# Step 6: Export Results to Shapefile\n",
        "## 6.1. Keep required columns and geometry\n",
        "columns_to_keep = [\"listing_density\", \"nature_density\", \"transport_density\", \"geometry\"]\n",
        "final_data = ward[columns_to_keep]\n",
        "\n",
        "## 6.2. Save to a new Shapefile\n",
        "final_data.to_file(\"data/final_results.shp\", driver=\"ESRI Shapefile\")"
      ],
      "id": "aa8a29f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| output: false\n",
        "\n",
        "# Step 7: Spatial Join and Filtering Airbnb Data\n",
        "## 7.1. Perform a spatial join: Airbnb data with ward boundaries\n",
        "listing_ward = gpd.sjoin(listing_spatial, ward, how=\"left\", predicate=\"intersects\")\n",
        "\n",
        "## 7.2. Select relevant columns to keep\n",
        "columns_to_keep = [\n",
        "    \"id\",\n",
        "    \"room_type\",\n",
        "    \"price\",\n",
        "    \"minimum_nights\",\n",
        "    \"calculated_host_listings_count\",\n",
        "    \"GSS_CODE\",\n",
        "    \"geometry\",\n",
        "]\n",
        "listing_ward = listing_ward[columns_to_keep]\n",
        "\n",
        "## 7.3. Preview the joined data\n",
        "print(listing_ward.head())\n",
        "\n",
        "## 7.4. Save the spatially joined data to a Shapefile\n",
        "listing_ward.to_file(\"data/listing_ward.shp\", driver=\"ESRI Shapefile\")"
      ],
      "id": "f18334d1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| output: false\n",
        "\n",
        "# Step 8: Calculate Average Price and Minimum Nights per Ward\n",
        "# --------------------------------------------------------------\n",
        "## 8.1 Calculate Average Price\n",
        "### 8.1.1 Group by GSS_CODE and calculate the average price\n",
        "avg_price = listing_ward.groupby(\"GSS_CODE\", as_index=False)[\"price\"].mean()\n",
        "avg_price.rename(columns={\"price\": \"avg_price\"}, inplace=True)\n",
        "\n",
        "### 8.1.2. Merge the average price back to the ward data\n",
        "ward[\"avg_price\"] = ward[\"GSS_CODE\"].map(avg_price.set_index(\"GSS_CODE\")[\"avg_price\"])\n",
        "\n",
        "### 8.1.3. Check the updated ward data\n",
        "print(ward.info())\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "## 8.2 Calculate Average Minimum Nights\n",
        "### 8.2.1 Group by GSS_CODE and calculate the average minimum nights\n",
        "avg_night = listing_ward.groupby(\"GSS_CODE\", as_index=False)[\"minimum_nights\"].mean()\n",
        "avg_night.rename(columns={\"minimum_nights\": \"avg_night\"}, inplace=True)\n",
        "\n",
        "### 8.2.2 Merge the average minimum nights back to the ward data\n",
        "ward[\"avg_night\"] = ward[\"GSS_CODE\"].map(avg_night.set_index(\"GSS_CODE\")[\"avg_night\"])\n",
        "\n",
        "### 8.2.3 Check the updated ward data\n",
        "print(ward.info())"
      ],
      "id": "a41ef974",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| output: false\n",
        "\n",
        "# Step 9: Calculate Average Host Listings and Entire Home Density\n",
        "# --------------------------------------------------------------\n",
        "## 9.1 Average Host Listings\n",
        "### 9.1.1 Calculate the average number of host listings per ward\n",
        "avg_host_listing = listing_ward.groupby(\"GSS_CODE\", as_index=False)[\n",
        "    \"calculated_host_listings_count\"\n",
        "].mean()\n",
        "avg_host_listing.rename(\n",
        "    columns={\"calculated_host_listings_count\": \"avg_host_listing\"}, inplace=True\n",
        ")\n",
        "\n",
        "### 9.1.2. Merge the average host listings back to the ward data\n",
        "ward[\"avg_host_listing\"] = ward[\"GSS_CODE\"].map(\n",
        "    avg_host_listing.set_index(\"GSS_CODE\")[\"avg_host_listing\"]\n",
        ")\n",
        "\n",
        "### 9.1.3. Check the updated ward data\n",
        "print(ward.info())\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "## 9.2 Calculate 'Entire Home' Density\n",
        "### 9.2.1 Filter rows where room_type contains 'Entire'\n",
        "entire_count = listing_ward[listing_ward[\"room_type\"].str.contains(\"Entire\")]\n",
        "entire_count = entire_count.groupby(\"GSS_CODE\").size().reset_index(name=\"entire_count\")\n",
        "\n",
        "### 9.2.2 Merge the count of 'Entire' listings back to the ward data\n",
        "ward = ward.merge(entire_count, on=\"GSS_CODE\", how=\"left\")\n",
        "\n",
        "### 9.2.3 Calculate the density of 'Entire' listings per ward area\n",
        "ward[\"entire_density\"] = ward[\"entire_count\"] / ward[\"area\"]\n",
        "\n",
        "### 9.2.4 Fill NaN values for wards with no 'Entire' listings\n",
        "ward[\"entire_count\"] = ward[\"entire_count\"].fillna(0)\n",
        "ward[\"entire_density\"] = ward[\"entire_density\"].fillna(0)\n",
        "\n",
        "### 9.2.5. Check the updated ward data\n",
        "print(ward.info())"
      ],
      "id": "d9c1f024",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| output: false\n",
        "\n",
        "# Step 10: Save Results to Shapefile\n",
        "## 10.1. Define the columns to retain\n",
        "columns_to_keep = [\n",
        "    \"listing_density\",\n",
        "    \"nature_density\",\n",
        "    \"transport_density\",\n",
        "    \"tourism_density\",\n",
        "    \"avg_price\",\n",
        "    \"avg_night\",\n",
        "    \"avg_host_listing\",\n",
        "    \"entire_density\",\n",
        "]\n",
        "\n",
        "## 10.2 Create a new dataset with selected columns and save as Shapefile\n",
        "gwr_data = ward[columns_to_keep + [\"geometry\"]]\n",
        "gwr_data.to_file(\"data/gwr.shp\", driver=\"ESRI Shapefile\")"
      ],
      "id": "e55fbb79",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| output: false\n",
        "\n",
        "# Step 11: Perform GWR\n",
        "# --------------------------------------------------------------\n",
        "## 11.1 Prepare GWR Input Data\n",
        "### 11.1.1 Read the Shapefile data for GWR\n",
        "gwr = gpd.read_file(\"data/gwr.shp\")\n",
        "\n",
        "### 11.1.2 Extract centroids and coordinates for GWR\n",
        "gwr[\"geometry\"] = gwr.centroid\n",
        "gwr[\"x\"] = gwr.geometry.x\n",
        "gwr[\"y\"] = gwr.geometry.y\n",
        "\n",
        "### 11.1.3 Define dependent variable (Y), independent variables (X), and coordinates\n",
        "Y = gwr[[\"listing_de\"]].values\n",
        "X = gwr[\n",
        "    [\n",
        "        \"nature_den\",\n",
        "        \"transport_\",\n",
        "        \"tourism_de\",\n",
        "        \"avg_price\",\n",
        "        \"avg_night\",\n",
        "        \"avg_host_l\",\n",
        "        \"entire_den\",\n",
        "    ]\n",
        "].values\n",
        "coords = gwr[[\"x\", \"y\"]].values"
      ],
      "id": "22b93916",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| output: false\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "## 11.2 Run GWR Model\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Set cache file\n",
        "cache_file = \"gwr_results_cache.pkl\"\n",
        "\n",
        "# Check cache\n",
        "if os.path.exists(cache_file):\n",
        "    print(\"Loading cached GWR results...\")\n",
        "    with open(cache_file, \"rb\") as f:\n",
        "        bw, results = pickle.load(f)\n",
        "else:\n",
        "    print(\"Running GWR model...\")\n",
        "    ### 11.2.1 Determine the optimal bandwidth\n",
        "    bw = Sel_BW(coords, Y, X).search()\n",
        "    \n",
        "    ### 11.2.2 Fit the GWR model\n",
        "    model = GWR(coords, Y, X, bw)\n",
        "    results = model.fit()\n",
        "    \n",
        "    ### 11.2.3 Save to cache\n",
        "    with open(cache_file, \"wb\") as f:\n",
        "        pickle.dump((bw, results), f)"
      ],
      "id": "ad8c95e5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "#| output: false\n",
        "\n",
        "### 11.2.4 Print summary of GWR results\n",
        "print(\"GWR Results Summary:\")\n",
        "print(f\"R²: {results.R2}\")\n",
        "print(f\"AIC: {results.aic}\")\n",
        "print(f\"Bandwidth: {bw}\")\n",
        "print(f\"Parameters shape: {results.params.shape}\")\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "## 11.3 Extract Local R² and Coefficients\n",
        "gwr['gwr_r2'] = results.localR2\n",
        "\n",
        "for i, col in enumerate(['intercept', 'nature_density', 'transport_density', 'tourism_density',\n",
        "                         'avg_price', 'avg_night', 'avg_host_listing', 'entire_density']):\n",
        "    gwr[f'gwr_{col}'] = results.params[:, i]"
      ],
      "id": "1466754a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "#| warning: false\n",
        "\n",
        "# Step 12: Visualisation\n",
        "## 12.1. Set up the figure with a 2x4 layout\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))  # 2 rows, 4 columns\n",
        "\n",
        "## 12.2. Define the variables and custom titles for each subplot\n",
        "variables = [\n",
        "    'gwr_r2', 'transport_density', 'tourism_density', 'nature_density',\n",
        "    'avg_price', 'avg_night', 'avg_host_listing', 'entire_density'\n",
        "]\n",
        "\n",
        "titles = [\n",
        "    'Local R² Distribution', 'Transport Density Coefficient', 'Tourism Density Coefficient', 'Nature Density Coefficient',\n",
        "    'Average Price Coefficient', 'Average Minimumn Nights Coefficient', 'Average Host Listings Coefficient',\n",
        "    'Entire Home Density Coefficient'\n",
        "]\n",
        "\n",
        "## 12.3. Loop through the variables and plot each on the corresponding axis\n",
        "for i, (var, title) in enumerate(zip(variables, titles)):\n",
        "    row, col = divmod(i, 4)  # Calculate row and column index for 2x4 layout\n",
        "    gwr.plot(column=f\"gwr_{var}\" if var != \"gwr_r2\" else var, \n",
        "             cmap='viridis', legend=True, ax=axes[row, col])  # Use viridis colormap\n",
        "    axes[row, col].set_title(title)  # Set custom title\n",
        "    axes[row, col].axis('off')  # Remove axes for clean plots\n",
        "\n",
        "## 12.4. Adjust layout and display the figure\n",
        "plt.figtext(0.5, -0.02, \n",
        "            'Figure 6: Spatial Distribution of GWR Results for Airbnb Listings Analysis', \n",
        "            ha='center', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "id": "c2fe6fd5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The GWR model demonstrates strong explanatory power, accounting for over 80% of the variation in non-compliant listings across London (R² ≥ 0.8) and 95% in concentrated central areas (R² = 0.95). Key factors driving non-compliance include public transport density and tourist attraction density.\n",
        "\n",
        "However, significant spatial heterogeneity exists. In the lower-left central region, public transport density shows a strong positive correlation, while the upper-right shows a moderate negative correlation. For tourist attraction density, the opposite pattern is observed: a strong negative correlation in the lower-left and a moderate positive correlation in the upper-right. This suggests that no single factor uniformly explains non-compliance across all areas.\n",
        "\n",
        "\n",
        "--- \n",
        "\n",
        "## Conclusion / Policy Recommendations\n",
        "We conducted a spatial analysis using Inside Airbnb data and found that illegal listings in London exhibit a clear clustering pattern. These listings significantly impact local housing market rental levels and vacancy rates, highlighting the necessity and reasonableness of the current 90-day short-term rental policy.\n",
        "\n",
        "Further multi-factor analysis reveals that while illegal listings show some correlation with factors such as host type, property type, minimum rental period, and price, their overall impact is limited. Instead, traffic network density and tourist density emerge as the key drivers of illegal listing distribution. However, their influence varies significantly across regions, indicating that a single regulatory policy cannot effectively address illegal activities in all areas.  \n",
        "\n",
        "**Policy Recommendations**  \n",
        "1. **Citywide Housing Database and Data Sharing**: We recommend that the government establish a citywide housing database and require short-term rental platforms to sign data-sharing agreements. These platforms should regularly upload housing information and usage data to support regulatory work.\n",
        "\n",
        "2. **Learning from European Cities**: Drawing on the experience of cities like Barcelona, short-term rental platforms should be compelled to remove unregistered listings and face stricter penalties for violations. [@Gianluca2025]\n",
        "\n",
        "3. **Differential Regional Regulation**: Building on London’s Housing Act 2016 [@Housing2016], we suggest implementing region-specific regulations. Based on data analysis, enforcement resources can be allocated more effectively to hotspot areas of illegal listings, optimizing patrols and rectification measures.\n",
        "\n",
        "These measures would improve regulatory efficiency and help reduce illegal activities across London’s diverse regions.\n",
        "\n",
        "--- \n",
        "\n",
        "## References"
      ],
      "id": "b4a6fe9d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (base)",
      "language": "python",
      "name": "base"
    },
    "jupytext": {
      "text_representation": {
        "extension": ".qmd",
        "format_name": "quarto",
        "format_version": "1.0",
        "jupytext_version": "1.16.4"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}