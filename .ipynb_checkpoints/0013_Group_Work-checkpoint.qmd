---
bibliography: bio.bib
csl: harvard-cite-them-right.csl
title: NA_Group2's Group Project
assess:
  group-date: '2024-12-17'
execute:
  echo: false
  freeze: true
format:
  html:
    code-copy: true
    code-link: true
    toc: true
    toc-title: On this page
    toc-depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
  pdf:
    include-in-header:
      text: |
        \addtokomafont{disposition}{\rmfamily}
    mainfont: Spectral
    sansfont: Roboto Flex
    monofont: Liberation Mono
    papersize: a4
    geometry:
      - top=20mm
      - left=25mm
      - right=25mm
      - bottom=20mm
      - heightrounded
    toc: false
    number-sections: false
    colorlinks: true
    highlight-style: github
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python (base)
    language: python
    name: base
---

```{python}
#| echo: false
import os
import pandas as pd
```

```{python}
#| echo: false
host = 'https://orca.casa.ucl.ac.uk'
path = '~jreades/data'
file = '20240614-London-listings.parquet'

if os.path.exists(file):
  df = pd.read_parquet(file)
else: 
  df = pd.read_parquet(f'{host}/{path}/{file}')
  df.to_parquet(file)
```

## Declaration of Authorship {.unnumbered .unlisted}

We, [NA_Group2], pledge our honour that the work presented in this assessment is our own. Where information has been derived from other sources, we confirm that this has been indicated in the work. Where a Large Language Model such as ChatGPT has been used we confirm that we have made its contribution to the final submission clear.

Date: 17/12/2024

Student Numbers: 4

## Word Count 

**Text Content**: 1,303   [[[[+++++ yezhen part and conclusion????????????]]]

**Visuals**: 6 figures × 150 per figure = 900 

**Total**: 2,500 [[[[[[[[[[[[]]]]]]]]]]]]]]


## Brief Group Reflection

| What Went Well                                      | What Was Challenging                               |  
|----------------------------------------------------|--------------------------------------------------|  
| Clear focus on the 90-day policy as the core theme.| InsideAirbnb lacks direct data on annual rental days, requiring estimation. |  
| Initial observations revealed significant policy violations.| Snapshot data is time-sensitive and may not fully capture temporal changes. |  
| Spatial clustering analysis highlighted policy-violating hotspots.| Logistic regression model had low explanatory power (R² = 0.05). |  
| Iterative refinement of models improved insights (GWLR R² = 0.55–0.96).| Residual analysis and Moran's I indicated spatial heterogeneity. |  
| Identified policy violations' impact on rent and vacancy rates.| Incorporating new variables (e.g., attraction density) was computationally intensive. |  
| Combined four quarters of snapshot data to mitigate time-sensitivity limitations.| Assessing the broader socioeconomic impact was limited by data availability. |

## Priorities for Feedback

**Code Consistency:** When working together, our code and methods often feel disconnected and lack coherence. How can we ensure clearer, more consistent approaches across the team?

**Accuracy of Policy Violation Estimates:** Given the limitations of InsideAirbnb data (e.g., no direct measure for annual rental days), we used proxies like reviews and minimum stay to estimate violations. We’d appreciate guidance on how to validate or improve this estimation method to reduce bias.

**Model Discrepancy and Improvement:** While GWLR significantly improved explanatory power (R² = 0.55–0.96), the initial logistic regression model had very low explanatory power (R² = 0.05). We would appreciate feedback on whether there are key factors missing in the initial model and how to avoid potential overfitting or over-reliance on local weights in the GWLR model.

{{< pagebreak >}}



## 1. Who collected the InsideAirbnb data?


The data was collected, integrated, and published by Inside Airbnb’s policy and housing researchers, led by founder Murray Cox, an activist using data-driven insights to address housing challenges [@inside_airbnb_about_nodate].

---

## 2. Why did they collect the InsideAirbnb data?


Inside Airbnb collects data to help communities understand Airbnb's impact on housing and neighborhoods. They aim to provide data that empower communities make informed decisions, manage short-term rentals, protect residents from negative effects like rising rents and housing shortages, and support collective efforts by residents and activists [@cox_inside_2023].

---

## 3. How did they collect it?


Inside Airbnb used Python-based web scraping to collect publicly available data from the Airbnb website [@alsudais_incorrect_2021]. The data is captured as a time-specific snapshot, including availability calendars (up to 365 days), reviews, names, photos, and other publicly visible details. After collection, the data was cleaned, analyzed, and aggregated. Airbnb’s categorization of unavailable nights (booked or blocked) and location anonymization were retained unchanged, while neighborhood names were refined using city-defined boundaries for accuracy [@inside_airbnb_about_nodate].

---

## 4. How does the method of collection (Q3) impact the completeness and/or accuracy of the InsideAirbnb data? How well does it represent the process it seeks to study, and what wider issues does this raise?


The Inside Airbnb data reveals key market trends and geographic distribution of listings, offering a foundation for analyzing short-term rental impacts on residential neighborhoods. But it has following limitations:  

1. The data only includes Airbnb listings, which may be removed or hidden due to business or regulatory pressures. For example, in 2015, Airbnb deleted over 1,000 entire-home listings in New York City [@cox_how_2016].

2. The data's snapshot nature cannot reflect ongoing changes, such as newly added or removed listings. Differences between dataset versions can also affect reliability and reproducibility if the version used is not specified.

3. The data excludes off-platform bookings, such as private arrangements to avoid platform fees or bypass rental day regulations. Additionally, booked dates are marked as unavailable, creating the false impression that popular listings are unrentable. These factors lead to an underestimation of actual bookings and occupancy rates.

4. Geolocation data is anonymized, shifting locations by up to 150 meters and scattering listings within buildings, which reduces accuracy for neighborhood-level analysis.

5. Airbnb sometimes assigns the same ID to both 'Experiences' and 'Listings,' causing 'Experience' reviews to be misclassified as 'Listing' reviews and leading to inaccuracies in review data [@alsudais_incorrect_2021].

---

## 5. What ethical considerations does the use of the InsideAirbnb data raise? 


1. **Preventing Misuse of Data for Commercial Gain**  
    Inside Airbnb’s data is meant to reveal the impacts of short-term rentals and to support local communities. However, malicious exploitation of the data—such as investors using it to identify high-profit areas for short-term rentals—could lead to negative consequences, such as increased property purchases for rentals, displacement of residents, and worsening housing crises. This undermines the platform’s mission and further harms vulnerable groups. Researchers and policymakers have a responsibility to ensure that their use of the data aligns with community interests and to avoid sharing data in ways that enable harmful commercial use.



3. **Protecting User Privacy**  
    Since the data is collected through web scraping, Airbnb hosts and guests may not have given consent for its use. Even anonymized geographic data can unintentionally expose sensitive information, especially in small communities or for standalone properties. Publicly identifying short-term rental activity could subject hosts and guests to harassment, discrimination, or financial loss. Research outputs should generalize findings to avoid unintentionally identifying individuals and use neutral language to prevent stigmatization of particular groups.

5. **Ensuring Representation of Vulnerable Communities: Promoting Equity in Data Interpretation**  
    While Inside Airbnb data is openly available, the ability to analyze and interpret it is often limited to technically skilled individuals, such as researchers or local activists. This can result in policies that prioritize the perspectives of these groups while neglecting the needs of underrepresented or vulnerable communities. To address this imbalance, researchers and policymakers should engage marginalized communities in the interpretation process, present findings in accessible formats, and ensure data-driven decisions promote social equity, as highlighted in Data Feminism [@dignazio_data_2020].

---

## 6. With reference to the InsideAirbnb data (*i.e.* using numbers, figures, maps, and descriptive statistics), what does an analysis of Hosts and the types of properties that they list suggest about the nature of Airbnb lettings in London? 

```{python}
#| echo: false
#| output: false
#Reading airbnb data 2013.12
import pandas as pd  
import numpy as np  
from scipy.stats import pearsonr, pointbiserialr, chi2_contingency  
import matplotlib.pyplot as plt  
from matplotlib.font_manager import FontProperties  
from matplotlib.colors import LinearSegmentedColormap  
from matplotlib.gridspec import GridSpec  
import seaborn as sns  
import os  # For creating directories

# loading data
file_path = 'data/listings_202312.csv' 
airbnb_data = pd.read_csv(file_path)  
 
print(airbnb_data.info()) 

#create plots directory
output_dir = "plots"  
if not os.path.exists(output_dir):  
    os.makedirs(output_dir) 
```

```{python}
#| echo: false
# estimate booking days per year by reviews and minimum nights
airbnb_data = airbnb_data[airbnb_data['availability_365'] > 0] 
airbnb_data['estimated_nights_booked'] = airbnb_data['reviews_per_month'] * 12 * airbnb_data['minimum_nights'] * 2  

#Data cleaning
# Replace NaN with 0
airbnb_data['estimated_nights_booked'] = airbnb_data['estimated_nights_booked'].fillna(0)
# Convert the column to integers
airbnb_data['estimated_nights_booked'] = airbnb_data['estimated_nights_booked'].astype(int)

airbnb_data.to_csv('data/processed_airbnb_data.csv', index=False)  
```

### The 90-Day Rule and Airbnb Commercialization  

Airbnb has reduced long-term rentals, raised rents, and driven commercialization in London’s housing market. The 90-day rule limits entire-home short-term rentals to 90 days per year without special planning permission [@greaterlondonauthority], but enforcement depends on self-reporting, which is weak. To address this, we use InsideAirbnb data to analyze booking patterns, host behavior, and property types to assess commercialization and compliance risks. Let's start by conducting an initial observation of the data:

```{python}
#| echo: false
#| warning: false
# 1. Data Cleaning and Preparation  
# -------------------------------  
# Select relevant columns and drop rows with missing values  
data = airbnb_data[['host_id', 'room_type', 'availability_365', 'calculated_host_listings_count',  
                    'reviews_per_month', 'minimum_nights', 'estimated_nights_booked',  
                    'price', 'latitude', 'longitude']].dropna()  

# Filter data where availability_365 is greater than 0  
data = data[data['availability_365'] > 0]  

import geopandas as gpd  
# Load the borough map as a GeoDataFrame  
borough_map = gpd.read_file("data/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp")

# Convert the cleaned data into a GeoDataFrame  
gdf = gpd.GeoDataFrame(  
    data,   
    geometry=gpd.points_from_xy(data['longitude'], data['latitude']),  
    crs="EPSG:4326"  # WGS84 coordinate system  
)  
# Reproject the GeoDataFrame to EPSG:27700 (British National Grid)  
gdf = gdf.to_crs("EPSG:27700")  
# Ensure both GeoDataFrames use the same CRS  
borough_map = borough_map.to_crs(gdf.crs)  
```

```{python}
#| echo: false
#| warning: false

# -------------------------------  
# 1. Setup the Main Figure Layout  
# -------------------------------  
fig = plt.figure(figsize=(16, 10))  # Main figure size  
gs = GridSpec(1, 1, figure=fig)  # Single grid for the map  

# Subplot positions and sizes  
x_offset = 0.75  
y_offsets = [0.82, 0.62, 0.42, 0.22]  
subplot_width, subplot_height = 0.18, 0.12  

# -------------------------------  
# 2. Plot the Map of Airbnb Listings  
# -------------------------------  
ax_map = fig.add_subplot(gs[0, 0])  
borough_map.plot(ax=ax_map, color='lightblue', edgecolor='darkblue', alpha=0.44)  

# Define room type colors  
room_type_colors = {  
    'Entire home/apt': '#FF7F7F',  
    'Private room': '#77DD77',  
    'Shared room': '#FDFD96',  
    'Hotel room': '#FFA07A'  
}  

# Plot Airbnb listings by room type  
for room_type, color in room_type_colors.items():  
    gdf[gdf['room_type'] == room_type].plot(  
        ax=ax_map, color=color, markersize=2, label=room_type, alpha=0.7  
    )  

# Add legend  
legend = ax_map.legend(  
    title="Room Type", loc='upper left', fontsize=10, frameon=True, edgecolor='lightgray'  
)  
legend.get_title().set_fontsize(10)  
legend.get_title().set_fontweight('bold')  

# Add map title  
plt.title('Airbnb Listings in London', fontsize=13)  

# Remove axes and borders for a cleaner map  
ax_map.set_xticks([]), ax_map.set_yticks([])  
for spine in ax_map.spines.values():  
    spine.set_visible(False)  

# -------------------------------  
# 3. Subplot 1: Estimated Booking Days  
# -------------------------------  
ax1 = fig.add_axes([x_offset, y_offsets[0], subplot_width, subplot_height])  

# Define bins and labels for booking days  
bins = [0, 30, 60, 89.999, 120, 150, 180, 210, 240, 270, 300, 330, 356]  
labels = ['0-30', '30-60', '60-90', '90-120', '120-150', '150-180', '180-210',  
          '210-240', '240-270', '270-300', '300-330', '330-356', '356+']  

# Assign colors based on booking days  
data['booking_color'] = data['estimated_nights_booked'].apply(  
    lambda x: 'blue' if x <= 89 else 'red'  
)  

# Plot histogram  
sns.histplot(  
    data=data, x='estimated_nights_booked', hue='booking_color',  
    palette={'blue': '#6A9FB5', 'red': '#FF6F61'}, multiple='dodge',  
    edgecolor='black', linewidth=0.5, binwidth=15, ax=ax1  
)  
ax1.set_title('1. Estimated Booking Days', fontsize=10, fontweight='bold')  
ax1.set_xlabel('Booking Days (0-356)', fontsize=8)  
ax1.set_ylabel('Listings', fontsize=8)  
ax1.tick_params(axis='x', rotation=45, labelsize=7)  
ax1.tick_params(axis='y', labelsize=7)  
ax1.set_xlim(0, 356)  

# Add vertical line at 90  
ax1.axvline(x=90, color='black', linestyle='--', linewidth=1)  

# Set custom x-axis ticks and labels  
ax1.set_xticks(bins)  
ax1.set_xticklabels(labels, rotation=45, fontsize=6.5)  

# Add custom legend  
ax1.legend(  
    handles=[  
        plt.Line2D([0], [0], color='#6A9FB5', lw=4, label='< 90'),  
        plt.Line2D([0], [0], color='#FF6F61', lw=4, label='>=90')  
    ],  
    title="Booking Days", loc='upper right', fontsize=8, title_fontsize=8  
)  
ax1.spines['top'].set_visible(False)  
ax1.spines['right'].set_visible(False)  

# -------------------------------  
# 4. Subplot 2: Listings per Host  
# -------------------------------  
ax2 = fig.add_axes([x_offset, y_offsets[1], subplot_width, subplot_height])  

# Group listings per host and create a new column for "10+"  
listings_per_host = data.groupby('host_id').size()  
listings_per_host = listings_per_host.apply(lambda x: x if x <= 10 else 11)  

# Plot histogram  
sns.histplot(  
    listings_per_host, kde=False, color='lightblue',  
    edgecolor='black', linewidth=0.5, binwidth=0.8, ax=ax2  
)  
ax2.set_title('2. Listings per Host', fontsize=10, fontweight='bold')  
ax2.set_xlabel('Number of Listings', fontsize=8)  
ax2.set_ylabel('Hosts', fontsize=8)  
ax2.tick_params(axis='x', labelsize=7)  
ax2.tick_params(axis='y', labelsize=7)  
ax2.set_xticks(range(1, 12))  
ax2.set_xticklabels([str(i) for i in range(1, 11)] + ['10+'])  
ax2.set_xlim(0.5, 11.5)  
ax2.spines['top'].set_visible(False)  
ax2.spines['right'].set_visible(False)  

# -------------------------------  
# 5. Subplot 3: Room Type Count  
# -------------------------------  
ax3 = fig.add_axes([x_offset, y_offsets[2], subplot_width, subplot_height])  

# Plot bar chart  
room_type_count = data['room_type'].value_counts()  
room_type_count.index = ['Entire home', 'Private room', 'Shared room', 'Hotel room']  
sns.barplot(  
    x=room_type_count.index, y=room_type_count.values,  
    palette=['#FF7F7F', '#AEC6CF', '#FDFD96', '#FFA07A'],  
    edgecolor='black', linewidth=0.5, width=0.4, ax=ax3  
)  
ax3.set_title('3. Room Type Count', fontsize=10, fontweight='bold')  
ax3.set_xlabel('Room Type', fontsize=8)  
ax3.set_ylabel('Count', fontsize=8)  
ax3.tick_params(axis='x', labelsize=7)  
ax3.tick_params(axis='y', labelsize=7)  
ax3.spines['top'].set_visible(False)  
ax3.spines['right'].set_visible(False)  

# -------------------------------  
# 6. Subplot 4: Minimum Nights Distribution  
# -------------------------------  
ax4 = fig.add_axes([x_offset, y_offsets[3], subplot_width, subplot_height])  

# Plot histogram  
sns.histplot(  
    data['minimum_nights'], bins=1000, kde=False,  
    color='lightcoral', edgecolor='black', linewidth=0.5, ax=ax4  
)  
ax4.set_title('4. Minimum Nights Distribution', fontsize=10, fontweight='bold')  
ax4.set_xlabel('Minimum Nights', fontsize=8)  
ax4.set_ylabel('Listings', fontsize=8)  
ax4.tick_params(axis='x', labelsize=7)  
ax4.tick_params(axis='y', labelsize=7)  
ax4.set_xlim(0, 40)  
ax4.spines['top'].set_visible(False)  
ax4.spines['right'].set_visible(False)  

# -------------------------------  
# 7. Add a Table Header  
# -------------------------------  
fig.suptitle('Figure 1: Airbnb Listings Analysis in London', fontsize=18, fontweight='bold', y=0.95)  

# -------------------------------  
# 8. Save the Figure  
# -------------------------------  
os.makedirs('plots', exist_ok=True)  # Create 'plots' folder if it doesn't exist  
plt.savefig('plots/figure_1_Airbnb_Listings_in_London.png', dpi=600, bbox_inches='tight')  

# -------------------------------  
# 9. Display the Figure  
# -------------------------------  
plt.show()
```

### Key Findings  

1. **Spatial Hotspot**:  
    Central London has the highest density of short-term rentals, with entire homes dominating high-demand, high-violation areas of the 90-day rule. (Figure 1, Map).

2. **Frequent Breaches of the 90-Day Limit**:  
    Booking data, based on reviews, pricing, and minimum nights [@insideairbnb2023], shows entire homes and hotel-like properties often exceed the 90-day limit, especially in commercialized areas (Figure 1, Panel 1). 

To address the limitations of using only snapshot data, we merged datasets from December 2023 to September 2024 and removed duplicates to ensure greater accuracy：

```{python}
#| echo: false
#| cache: true


# -------------------------------  
# 1. Load and Combine Data  
# -------------------------------  
# Define the folder and file names  
data_folder = "data"  
file_names = ["listings_20243.csv", "listings_20246.csv", "listings_20249.csv"]  

# Load and combine the datasets  
data_list = [pd.read_csv(os.path.join(data_folder, file)) for file in file_names]  
combined_data = pd.concat(data_list, ignore_index=True)  

# Combine with airbnb_data (2023 data)  
all_data = pd.concat([combined_data, airbnb_data], ignore_index=True)  

# -------------------------------  
# 2. Data Cleaning  
# -------------------------------  
# Filter out listings with no availability  
all_data = all_data[all_data['availability_365'] > 0]  

# Calculate estimated booking days  
all_data['estimated_nights_booked'] = (  
    all_data['reviews_per_month'] * 12 * all_data['minimum_nights'] * 2  
)  
all_data['estimated_nights_booked'] = all_data['estimated_nights_booked'].fillna(0).astype(int)  

# Remove duplicates  
all_data = all_data.drop_duplicates()  

# -------------------------------  
# 3. Create 'host_type' Column  
# -------------------------------  
# Ensure 'calculated_host_listings_count' exists  
if 'calculated_host_listings_count' not in all_data.columns:  
    raise KeyError("'calculated_host_listings_count' column is missing from the DataFrame.")  

# Create 'host_type' column  
all_data['host_type'] = all_data['calculated_host_listings_count'].apply(  
    lambda x: 'Single' if x == 1 else 'Multi'  
)  

# -------------------------------  
# 4. Calculate Metrics  
# -------------------------------  
# Host type proportions  
host_proportions = all_data['host_type'].value_counts(normalize=True) * 100  
host_proportions = host_proportions.round(2)  

# Average booking days by host type  
host_avg_booking_days = all_data.groupby('host_type')['estimated_nights_booked'].mean().round(2)  

# Room type proportions  
room_type_proportions = all_data['room_type'].value_counts(normalize=True) * 100  
room_type_proportions = room_type_proportions.round(2)  

# Average booking days by room type  
room_type_avg_days = all_data.groupby('room_type')['estimated_nights_booked'].mean().round(2)  

# Average price and minimum nights by host type and room type  
avg_price_heatmap = all_data.pivot_table(index='host_type', columns='room_type', values='price', aggfunc='mean').round(2)  
min_nights_heatmap = all_data.pivot_table(index='host_type', columns='room_type', values='minimum_nights', aggfunc='mean').round(2)  
```

```{python}
#| echo: false
#| cache: true
# 5. Visualizations  
# -------------------------------  
fig = plt.figure(figsize=(16, 14))  

# Main title  
fig.suptitle('Figure 2: Analysis of Airbnb Listings by Host and Room Types', fontsize=16, fontweight='bold', y=0.95)  

# First Row: Tables  
# Left: Host Type Table  
ax1 = fig.add_axes([0.1, 0.7, 0.35, 0.15])  
ax1.axis('off')  
host_table_data = pd.DataFrame({  
    'Host Type': host_proportions.index,  
    'Proportion (%)': host_proportions.values,  
    'Avg Booking Days': host_avg_booking_days.values  
})  
host_table = ax1.table(  
    cellText=host_table_data.values,  
    colLabels=host_table_data.columns,  
    cellLoc='center',  
    loc='center',  
    bbox=[0, 0, 1, 1]  
)  
host_table.auto_set_font_size(False)  
host_table.set_fontsize(12)  
for key, cell in host_table.get_celld().items():  
    if key[0] == 0:  # Header row  
        cell.set_text_props(weight='bold')  
        cell.set_height(0.1)  
    else:  
        cell.set_height(0.15)  

# Add left-side title  
fig.text(0.15, 0.87, 'Host Type Distribution and Booking Trends', fontsize=13, fontweight='bold')  

# Right: Room Type Table  
ax2 = fig.add_axes([0.55, 0.7, 0.35, 0.15])  
ax2.axis('off')  
room_table_data = pd.DataFrame({  
    'Room Type': room_type_proportions.index,  
    'Proportion (%)': room_type_proportions.values,  
    'Avg Booking Days': room_type_avg_days.values  
})  
room_table = ax2.table(  
    cellText=room_table_data.values,  
    colLabels=room_table_data.columns,  
    cellLoc='center',  
    loc='center',  
    bbox=[0, 0, 1, 1]  
)  
room_table.auto_set_font_size(False)  
room_table.set_fontsize(12)  
for key, cell in room_table.get_celld().items():  
    if key[0] == 0:  # Header row  
        cell.set_text_props(weight='bold')  
        cell.set_height(0.1)  
    else:  
        cell.set_height(0.15)  

# Add right-side title  
fig.text(0.6, 0.87, 'Room Type Distribution and Booking Trends', fontsize=13, fontweight='bold')  

# Second Row: Bar Charts  
# Left: Host Type Bar Chart  
ax3 = fig.add_axes([0.1, 0.45, 0.35, 0.2])  
host_avg_booking_days.plot(kind='bar', color=sns.color_palette("Pastel1")[:2], edgecolor='black', ax=ax3)  
ax3.set_title('Average Estimated Booking Days by Host Type', fontsize=10, fontweight='bold')  
ax3.set_xlabel('Host Type')  
ax3.set_ylabel('Average Booking Days')  

# Right: Room Type Bar Chart  
ax4 = fig.add_axes([0.55, 0.45, 0.35, 0.2])  
room_type_colors = sns.color_palette("Pastel1")[:len(room_type_avg_days)]  
room_type_avg_days.plot(kind='bar', color=room_type_colors, edgecolor='black', ax=ax4)  
ax4.set_title('Average Estimated Booking Days by Room Type', fontsize=10, fontweight='bold')  
ax4.set_xlabel('Room Type')  
ax4.set_ylabel('Average Booking Days')  

# Add dividing line  
fig.add_axes([0.49, 0.4, 0.01, 0.5]).plot([0, 0], [0, 1], linestyle='--', color='black', linewidth=1)  
plt.gca().axis('off')  

# Third Row: Heatmaps  
# Left: Average Price Heatmap  
ax5 = fig.add_axes([0.1, 0.1, 0.4, 0.23])  
sns.heatmap(avg_price_heatmap, annot=True, fmt=".2f", cmap="Blues", cbar=True, ax=ax5)  
ax5.set_title('Average Price by Host and Room Types', fontsize=10, fontweight='bold')  

# Right: Minimum Nights Heatmap  
slightly_higher_saturation_reds = LinearSegmentedColormap.from_list(  
    "slightly_higher_saturation_reds", ["#fff5f5", "#fcd9d9", "#f8a8a8", "#f47474"]  
)  
ax6 = fig.add_axes([0.55, 0.1, 0.4, 0.22])  
sns.heatmap(min_nights_heatmap, annot=True, fmt=".2f", cmap=slightly_higher_saturation_reds, cbar=True, ax=ax6)  
ax6.set_title('Minimum Nights by Host and Room Types', fontsize=10, fontweight='bold')  

# Save and display  
plt.savefig('plots/figure_2_Airbnb_Listings_Analysis.png', dpi=600, bbox_inches='tight')  
plt.show()
```

3. **Multi-Listing Hosts Dominate the Market**:  
    Multi-listing hosts manage 62.38% of Airbnb listings with longer average booking durations than single-listing. They primarily operate entire and hotel-like properties, which are highly commercialized with shorter stays and higher prices (Figure 2, Top Left; heatmap).

4. **Entire Homes are the Highest Risk**:  
    Entire homes (66.97% of listings) have the highest average booking days and are most likely to breach the 90-day rule, followed by hotel-like properties with short minimum stays (2.13 nights) and high commercialization.(Figure 2, Top Right; heatmap).


### Initial conclusion 
InsideAirbnb data reveals London’s highly commercialized Airbnb market with significant 90-day rule violations, concentrated in high-risk areas. Multi-listing hosts and commercialized properties are key drivers, requiring further spatial and regression analysis to assess impacts and improve enforcement.

---  

## 7. Drawing on your previous answers, and supporting your response with evidence (*e.g.* figures, maps, EDA/ESDA, and simple statistical analysis/models drawing on experience from, e.g., CASA0007), how *could* the InsideAirbnb data set be used to inform the regulation of Short-Term Lets (STL) in London? 


We decided to further analyze the December 2023 Inside Airbnb dataset to provide evidence on the link between Airbnb activity and policy violations. The next steps are:

1. **Identifying Violation Hotspots and Impacts:**  
    Spatial clustering will identify areas with high concentrations of 90-day rule violations. These hotspots will be analyzed for impacts on local communities, focusing on rising rents, reduced housing availability—issues tied to Airbnb-driven gentrification [@smith2006].

2. **Analyzing Drivers of Violations:**  
   Regression analysis helps identify key factors driving these violations (e.g. property type, host behavior, pricie and location...)

This approach will highlight the need for effective regulation, and clarify enforcement priorities to enhance the efficiency of enforcement efforts.

### Spatial Analysis of Policy Violations and Local Impacts  

1. **Hotspot Identification:** 
We analyzed the spatial clustering of Airbnb rule-breaking properties using Moran’s I and LISA cluster maps (Figure 3). High-High clusters were found in central boroughs, such as Westminster, and eastern areas like Hackney, where violations are linked to a combination of high tourism demand, profitability of short-term rentals, and housing market pressures [@bosma2024].  

```{python}
#| echo: false
#| output: false
#| cache: true
import warnings
warnings.filterwarnings('ignore')
from libpysal.weights import Queen  

# Read the data  
file_path = 'data/listings.csv'    #which is the path in this repository  
airbnb_data = pd.read_csv(file_path)

# Calculate the estimation of nights booked for each listing
airbnb_data = airbnb_data[airbnb_data['availability_365'] > 0] 
airbnb_data['estimated_nights_booked'] = airbnb_data['reviews_per_month'] * 12 * airbnb_data['minimum_nights'] * 2 

# Data cleaning: assign the estimated nights booked to each borough
# Replace NaN with 0
airbnb_data['estimated_nights_booked'] = airbnb_data['estimated_nights_booked'].fillna(0)

# Convert the column to integers
airbnb_data['estimated_nights_booked'] = airbnb_data['estimated_nights_booked'].astype(int)

#Count the number of listings in each borough using 'neighbourhood' column
borough_counts = airbnb_data['neighbourhood'].value_counts()

# Filter the DataFrame to include only rows where estimated_nights_booked is greater than 90
filtered_data = airbnb_data[airbnb_data['estimated_nights_booked'] > 90]

#Count the number of listings with estimation of nights booked larger than 90 days in each borough
borough_counts_90 = filtered_data['neighbourhood'].value_counts()

# Merge the two series into a DataFrame
combined_data = pd.concat([borough_counts, borough_counts_90], axis=1, keys=['Total_listings', 'More_than_90'])

# Calculate the ratio of listings with more than 90 booked nights per total listings
combined_data['Ratio_of_more_than_90'] = combined_data['More_than_90'] / combined_data['Total_listings']

# Fill any NaN values that might occur if there are boroughs with no listings > 90 nights
combined_data['Ratio_of_more_than_90'] = combined_data['Ratio_of_more_than_90'].fillna(0)

# Data formatting and round to four decimal places
combined_data['Ratio_of_more_than_90'] = combined_data['Ratio_of_more_than_90'].apply(lambda x: round(x, 4))

# Rename the index label to 'Borough_name'
combined_data.index.rename('Borough_name', inplace=True)

# Load the borough codes
borough_code_file_path = 'data/borough_name_code.csv'
borough_codes = pd.read_csv(borough_code_file_path)

# Reset index in combined_data to turn the index into a regular column
combined_data.reset_index(inplace=True)
borough_codes.reset_index(inplace=True)

#Combine the ratio data and borough name with borough code by borough name
combined_data = pd.merge(combined_data, borough_codes[['Borough_name', 'Borough_code']], on='Borough_name', how='left')

# Set 'Borough_name' back as the index
combined_data.set_index('Borough_name', inplace=True)

# Save the updated DataFrame
combined_data.to_csv('data/borough_listings_ratio.csv', index=True)
```

```{python}
#| echo: false
#| output: false
#| cache: true
# Moran analysis
from esda.moran import Moran  
# Load data
ratio = pd.read_csv("data/borough_listings_ratio.csv")
borough = gpd.read_file("data/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp")

# merge
borough_ratio = borough.merge(ratio, left_on="GSS_CODE", right_on="Borough_code")

# Calculate neighbors using Queen contiguity
weights = Queen.from_dataframe(borough_ratio)
weights.transform = 'r'  # Row-standardize the weights

os.makedirs('plots/raw', exist_ok=True)

# Global Moran's I
y = borough_ratio['Ratio_of_more_than_90']
moran = Moran(y, weights)
print(f"Global Moran's I: {moran.I:.3f}")
print(f"P-value: {moran.p_sim:.3f}")

# Moran Plot
def moran_plot(y, weights):
    lag = weights.sparse.dot(y)
    slope, intercept = np.polyfit(y, lag, 1)
    
    plt.figure(figsize=(10, 5))
    plt.scatter(y, lag)
    plt.plot(y, slope * y + intercept, 'r')
    plt.xlabel('Ratio of more than 90')
    plt.ylabel('Spatially Lagged Ratio')
    plt.title("Moran Plot of rule breaking Airbnbs")
    plt.axvline(y.mean(), color='gray', linestyle='--')
    plt.axhline(lag.mean(), color='gray', linestyle='--')
    plt.savefig('plots/raw/Moran_rule_breaking.png') 
    plt.show()

moran_plot(y, weights)
```

```{python}
#| echo: false
#| output: false
#| cache: true
# Local Moran's I
from esda.moran import Moran_Local 
local_moran = Moran_Local(y, weights)
borough_ratio['Ii'] = local_moran.Is
borough_ratio['p_value'] = local_moran.p_sim

# Plot Local Moran's I
fig, ax = plt.subplots(figsize=(12, 8))
borough_ratio.plot(column='Ii', legend=True, ax=ax)
plt.title("Local Moran's I Statistics")
plt.axis('off')
plt.show()

# LISA Cluster Map
sig = 0.1
labels = ['Not Significant', 'Low-Low', 'Low-High', 'High-Low', 'High-High']
colors = ['white', 'blue', 'lightblue', 'pink', 'red']

# Standardize the variable of interest
y_std = (y - y.mean()) / y.std()
lag_std = weights.sparse.dot(y_std)

# Create significance masks
sig_mask = local_moran.p_sim < sig

# Create cluster categories
borough_ratio['quadrant'] = np.zeros(len(y))
borough_ratio.loc[sig_mask, 'quadrant'] = np.where(y_std < 0,
    np.where(lag_std < 0, 1, 2),
    np.where(lag_std < 0, 3, 4))[sig_mask]

# Plot LISA clusters
fig, ax = plt.subplots(figsize=(10, 10))
borough_ratio.plot(column='quadrant', categorical=True, k=5, cmap='Paired',
                  legend=True, ax=ax)
plt.title('LISA Cluster Map of rule breaking Airbnbs')
plt.axis('off')
plt.savefig('plots/raw/LISA_rule_breaking.png') 
plt.show()

# Additional analysis plots
plt.figure(figsize=(10, 6))
plt.hist(y, bins=20)
plt.title('Distribution of Ratio_of_more_than_90')
plt.xlabel('Value')
plt.show()

print(y.describe())
# print(local_moran.Is.describe())
print(pd.Series(local_moran.Is).describe())

print(f"Number of significant clusters: {(local_moran.p_sim < 0.1).sum()}")
```

```{python}
#| echo: false
#| output: false
# Distance-based weights (20km)
from libpysal.weights import KNN  
centroids = borough_ratio.geometry.centroid
coords = np.column_stack((centroids.x, centroids.y))
knn = KNN.from_dataframe(borough_ratio, k=4)  # Approximate 20km neighbors
knn.transform = 'r'

# Calculate Local Moran's I with distance weights
local_moran_dist = Moran_Local(y, knn)

# Add results to GeoDataFrame
borough_ratio['Ii_dist'] = local_moran_dist.Is

# Plot results with distance-based weights
fig, ax = plt.subplots(figsize=(12, 8))
borough_ratio.plot(column='Ii_dist', legend=True, ax=ax)
plt.title("Local Moran Statistic (Distance-based)")
plt.axis('off')
plt.show()
```

```{python}
#| echo: false
#| output: false
#| cache: true
from libpysal.weights import Queen, lag_spatial
from esda.moran import Moran_BV, Moran_Local_BV

# load data
connect = pd.read_csv("data/connect.csv")
borough = gpd.read_file("data/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp")

# merge the data
borough_connect = borough.merge(connect, left_on="GSS_CODE", right_on="Borough_code")
```

```{python}
#| echo: false
#| output: false
#| cache: true
# analyse the spatial autocorrelation of monthly rent and airbnbs breaking the rule
# Variables
var1 = 'Monthly_rent_2023'
var2 = 'Ratio_of_more_than_90'

# Check for and handle missing data
borough_connect.dropna(subset=[var1, var2], inplace=True)

# Create weights and row-standardize them
weights = Queen.from_dataframe(borough_connect, use_index=True)
weights.transform = 'r'

# Bivariate Moran's I
moran_bv = Moran_BV(borough_connect[var1], borough_connect[var2], weights)
print(f"Bivariate Moran's I between {var1} and {var2}: {moran_bv.I:.3f}")
print(f"p-value: {moran_bv.p_sim:.3f}")

# Bivariate Moran Plot
fig, ax = plt.subplots(figsize=(10, 5))
spatial_lag_var2 = lag_spatial(weights, borough_connect[var2])  # Calculate the spatial lag of var2
scatter = ax.scatter(borough_connect[var1], spatial_lag_var2, color='blue', edgecolor='k', alpha=0.7)
fit = np.polyfit(borough_connect[var1], spatial_lag_var2, 1)
ax.plot(borough_connect[var1], np.polyval(fit, borough_connect[var1]), color='red', linestyle='--', linewidth=1)
ax.set_title('Bivariate Moran Scatter Plot monthly rent and rule breaking Airbnbs')
ax.set_xlabel(var1)
ax.set_ylabel(f"Spatial Lag of {var2}")
plt.savefig('plots/raw/Moran_monthly_rent.png')
plt.show()

# Bivariate Local Moran's I
local_moran_bv = Moran_Local_BV(borough_connect[var1], borough_connect[var2], weights)

# LISA Plot (Bivariate)
fig, ax = plt.subplots(figsize=(10, 10))
borough_connect.assign(cl=local_moran_bv.q).plot(column='cl', categorical=True, 
                                                 cmap='Paired', linewidth=0.1, ax=ax, 
                                                 edgecolor='white', legend=True)
labels = ['Not Significant', 'Low-Low', 'Low-High', 'High-Low', 'High-High']
legend = ax.get_legend()
if legend:
    legend.set_bbox_to_anchor((1, 1))
    legend.set_title('Cluster Type')
    for text, label in zip(legend.get_texts(), labels):
        text.set_text(label)

ax.set_title('Bivariate LISA Cluster Map of monthly rent and rule breaking Airbnbs')
ax.set_axis_off()
plt.savefig('plots/raw/LISA_monthly_rent.png')
plt.show()
```

```{python}
#| echo: false
#| output: false
#| cache: true
# analyse the spatial autocorrelation of vacant ratio and airbnbs breaking the rule
# Variables
var1 = 'Vacant_Ratio'
var2 = 'Ratio_of_more_than_90'

# Check for and handle missing data
borough_connect.dropna(subset=[var1, var2], inplace=True)

# Create weights and row-standardize them
weights = Queen.from_dataframe(borough_connect, use_index=True)
weights.transform = 'r'

# Bivariate Moran's I
moran_bv = Moran_BV(borough_connect[var1], borough_connect[var2], weights)
print(f"Bivariate Moran's I between {var1} and {var2}: {moran_bv.I:.3f}")
print(f"p-value: {moran_bv.p_sim:.3f}")

# Bivariate Moran Plot
fig, ax = plt.subplots(figsize=(10, 5))
spatial_lag_var2 = lag_spatial(weights, borough_connect[var2])  # Calculate the spatial lag of var2
scatter = ax.scatter(borough_connect[var1], spatial_lag_var2, color='blue', edgecolor='k', alpha=0.7)
fit = np.polyfit(borough_connect[var1], spatial_lag_var2, 1)
ax.plot(borough_connect[var1], np.polyval(fit, borough_connect[var1]), color='red', linestyle='--', linewidth=1)
ax.set_title('Bivariate Moran Scatter Plot of vacant ratio and rule breaking Airbnbs')
ax.set_xlabel(var1)
ax.set_ylabel(f"Spatial Lag of {var2}")
plt.savefig('plots/raw/Moran_vacant_ratio.png')
plt.show()

# Bivariate Local Moran's I
local_moran_bv = Moran_Local_BV(borough_connect[var1], borough_connect[var2], weights)

# LISA Plot (Bivariate)
fig, ax = plt.subplots(figsize=(10, 10))
borough_connect.assign(cl=local_moran_bv.q).plot(column='cl', categorical=True, 
                                                 cmap='Paired', linewidth=0.1, ax=ax, 
                                                 edgecolor='white', legend=True)
labels = ['Not Significant', 'Low-Low', 'Low-High', 'High-Low', 'High-High']
legend = ax.get_legend()
if legend:
    legend.set_bbox_to_anchor((1, 1))
    legend.set_title('Cluster Type')
    for text, label in zip(legend.get_texts(), labels):
        text.set_text(label)

ax.set_title('Bivariate LISA Cluster Map of vacant ratio and rule breaking Airbnbs')
ax.set_axis_off()
plt.savefig('plots/raw/LISA_vacant_ratio.png')
plt.show()
```

```{python}
#| echo: false
#| output: false
# Plotting the combined figure showing the rusults of Moran scatter plot and LISA cluster map
from PIL import Image, ImageDraw, ImageFont

# Paths to the images
morans = ['plots/raw/Moran_rule_breaking.png', 'plots/raw/Moran_monthly_rent.png', 'plots/raw/Moran_vacant_ratio.png']
lisas = ['plots/raw/LISA_rule_breaking.png', 'plots/raw/LISA_monthly_rent.png', 'plots/raw/LISA_vacant_ratio.png']

# Load all images
images = [Image.open(img) for img in morans + lisas]

# Calculate total width and height for the new image
total_width = images[0].width * 3
max_height = images[0].height + images[3].height 

# Create a new image with the appropriate size
new_im = Image.new('RGB', (total_width, max_height))

# Paste each Moran plot into the new image
for i, img in enumerate(images[:3]):  # First three are Moran plots
    new_im.paste(img, (img.width * i, 0))

# Paste each LISA plot into the new image
for i, img in enumerate(images[3:]):  # Last three are LISA plots
    new_im.paste(img, (img.width * i, images[0].height))  # Paste below the Moran plots

new_im.save('plots/combined_of_Moran_and_LISA.png')
```

### Figure 3: Results of Moran and LISA analysis of rule breaking Airbnbs, monthly rent and vacancy ratio

![](plots/combined_of_Moran_and_LISA.png)

2. **Housing Market Impacts**  
To quantify these impacts, we applied SAR and GWR models (Figure 4). The SAR analysis showed that violations contributed to rising rents and increased vacancy rates, with the strongest effects observed in central areas where tourism dominates and in eastern boroughs with emerging rental markets. GWR results highlighted spatial variability, with the highest rent surges in central London and higher vacancy rates in eastern boroughs.   Similar findings as [@jain2021nowcasting].

```{python}
#| echo: false
#| output: false
# SAR model
from spreg import ML_Lag
# Import data
data = pd.read_csv("data/connect.csv")
shp = gpd.read_file("data/statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp")

# Merge data and transform coordinate system
zone = shp.merge(data, left_on="GSS_CODE", right_on="Borough_code")
zone = zone.to_crs("EPSG:27700")

# Check and remove missing values
columns = ['Monthly_rent_2023', 'Vacant_Ratio', 'Ratio_of_more_than_90']
print("Missing values:\n", zone[columns].isna().sum())
zone = zone.dropna(subset=columns)

# Construct spatial weights matrix
w = Queen.from_dataframe(zone)
w.transform = 'r'

# Prepare variables
y = zone['Ratio_of_more_than_90'].values.reshape(-1, 1)
X = zone[['Monthly_rent_2023', 'Vacant_Ratio']].values

# Fit Spatial Lag Model
sar_model = ML_Lag(y, X, w=w,
                   name_y='Ratio_of_more_than_90',
                   name_x=['Monthly_rent_2023', 'Vacant_Ratio'],
                   name_w='w')

# Output model results
print("=== SAR Model Results ===")
print(sar_model.summary)

# Visualize residuals
zone['residuals'] = sar_model.u
fig, ax = plt.subplots(figsize=(8, 6))
zone.plot(column='residuals', cmap='viridis', legend=True, ax=ax)
plt.title("SAR Model Residuals")
plt.axis('off')
plt.show()
```

```{python}
#| echo: false
#| output: false
#| include: false
#| cache: true
# GWR model
from mgwr.gwr import GWR
from mgwr.sel_bw import Sel_BW
zone = zone.to_crs("EPSG:27700")
zone['centro'] = zone.geometry.centroid
zone['X'] = zone['centro'].x
zone['Y'] = zone['centro'].y
g_y_rent = zone['Monthly_rent_2023'].values.reshape((-1, 1))
g_X_rent = zone[['Ratio_of_more_than_90']].values
g_coords = list(zip(zone['X'], zone['Y']))

# Automatically set bw_min and bw_max based on the number of observations
n_obs = len(g_coords)  # Number of observations
bw_min = 2  # Minimum bandwidth, should be a positive integer
bw_max = max(bw_min, n_obs - 1)  # Ensures bw_max does not exceed n_obs - 1

# Initialize bandwidth selector with dynamic bandwidth settings
gwr_selector_rent = Sel_BW(g_coords, g_y_rent, g_X_rent, fixed=False)

# Search for optimal bandwidth using the golden section search method
gwr_bw_rent = gwr_selector_rent.search(search_method='golden_section', criterion='AICc', bw_min=bw_min, bw_max=bw_max)
print('Optimal Bandwidth Size for Rent:', gwr_bw_rent)

# Fit GWR model with the determined optimal bandwidth
gwr_results_rent = GWR(g_coords, g_y_rent, g_X_rent, gwr_bw_rent, fixed=False, kernel='bisquare').fit()
```

```{python}
#| echo: false
#| output: false
#| include: false
g_coords = list(zip(zone['X'], zone['Y']))

# Define independent and dependent variables for the Vacant_Ratio model
g_y_vacant = zone['Vacant_Ratio'].values.reshape((-1, 1))
g_X_vacant = zone[['Ratio_of_more_than_90']].values

# Automatically set bw_min and bw_max based on the number of observations
n_obs = len(g_coords)  # Number of observations
bw_min = 2  # Minimum bandwidth, should be a positive integer
bw_max = max(bw_min, n_obs - 1)  # Ensures bw_max does not exceed n_obs - 1

# Initialize bandwidth selector with dynamic bandwidth settings for Vacant_Ratio
gwr_selector_vacant = Sel_BW(g_coords, g_y_vacant, g_X_vacant, fixed=False)

# Search for optimal bandwidth using the golden section search method for Vacant_Ratio
gwr_bw_vacant = gwr_selector_vacant.search(search_method='golden_section', criterion='AICc', bw_min=bw_min, bw_max=bw_max)
print('Optimal Bandwidth Size for Vacant Ratio:', gwr_bw_vacant)

# Fit GWR model with the determined optimal bandwidth for Vacant_Ratio
gwr_results_vacant = GWR(g_coords, g_y_vacant, g_X_vacant, gwr_bw_vacant, fixed=False, kernel='bisquare').fit()
print(gwr_results_vacant.summary())
```

```{python}
#| echo: false
#| output: false
zone['coefficient'] = gwr_results_rent.params[:, 1]  # Add coefficients
zone['t_values'] = gwr_results_rent.tvalues[:, 1]  # Add t-values
```

```{python}
#| echo: false
#| output: false
#| include: false
# Define the variable names to be visualized, corresponding to the regression results added
var_names = ['coefficient']  # Adjust this if more variables from the model should be visualized

fig, axes = plt.subplots(1, len(var_names), figsize=(12, 3))

# Ensure `axes` is iterable
if len(var_names) == 1:
    axes = [axes]

for i, var in enumerate(var_names):
    ax = axes[i]  # Access each subplot axis
    zone.plot(column=var, cmap='viridis', legend=True, ax=ax, edgecolor='white', legend_kwds={'label': "Coefficient value"})
    ax.set_title(f'Regression Coefficients for {var}')
    ax.set_axis_off()

    # Highlight non-significant areas based on a significance threshold
    threshold = 1.96
    non_significant = zone['t_values'].abs() < threshold  # Ensuring the use of absolute value for significance checking
    zone.loc[non_significant].plot(ax=ax, color='lightgrey', edgecolor='white')

plt.tight_layout()
plt.show()
```

```{python}
#| echo: false
#| output: false
# Fit GWR for Monthly_rent_2023
gwr_model_rent = GWR(g_coords, zone['Monthly_rent_2023'].values.reshape((-1, 1)),
                     zone[['Ratio_of_more_than_90']].values.reshape((-1, 1)), gwr_bw_rent).fit()

# Fit GWR for Vacant_Ratio
gwr_model_vacant = GWR(g_coords, zone['Vacant_Ratio'].values.reshape((-1, 1)),
                       zone[['Ratio_of_more_than_90']].values.reshape((-1, 1)), gwr_bw_vacant).fit()

# Extract coefficients and t-values for each model
rent_coefs = pd.DataFrame(gwr_model_rent.params, columns=['Intercept', 'Effect_of_Ratio_of_more_than_90_on_Rent'])
rent_tvals = pd.DataFrame(gwr_model_rent.tvalues, columns=['t_Intercept', 't_Effect_on_Rent'])

vacant_coefs = pd.DataFrame(gwr_model_vacant.params, columns=['Intercept', 'Effect_of_Ratio_of_more_than_90_on_Vacancy'])
vacant_tvals = pd.DataFrame(gwr_model_vacant.tvalues, columns=['t_Intercept', 't_Effect_on_Vacancy'])
```

```{python}
#| echo: false
#| output: false
# Add results directly to zone GeoDataFrame
zone['Rent_Effect'] = rent_coefs['Effect_of_Ratio_of_more_than_90_on_Rent']
zone['Vacancy_Effect'] = vacant_coefs['Effect_of_Ratio_of_more_than_90_on_Vacancy']

# Check significance and add to zone
zone['Significant_Rent'] = rent_tvals['t_Effect_on_Rent'].abs() > 1.96
zone['Significant_Vacancy'] = vacant_tvals['t_Effect_on_Vacancy'].abs() > 1.96
```

```{python}
#| echo: false
#| output: false
#| include: false
fig, ax = plt.subplots(1, 2, figsize=(12, 6))

# Plot for Rent
zone.plot(column='Rent_Effect', cmap='viridis', ax=ax[0], legend=True,
          legend_kwds={'label': "Effect on Rent"})
zone[~zone['Significant_Rent']].plot(color='lightgrey', ax=ax[0])
ax[0].set_title('Effect of Ratio_of_more_than_90 on Rent')
ax[0].set_axis_off()

# Plot for Vacancy
zone.plot(column='Vacancy_Effect', cmap='viridis', ax=ax[1], legend=True,
          legend_kwds={'label': "Effect on Vacancy"})
zone[~zone['Significant_Vacancy']].plot(color='lightgrey', ax=ax[1])
ax[1].set_title('Effect of Ratio_of_more_than_90 on Vacancy')
ax[1].set_axis_off()

plt.tight_layout()
plt.show()
```

```{python}
#| echo: false
#| output: false
#| include: false
#| cache: true
# combing the plots to a new plot
zone['residuals'] = sar_model.u

# Create a figure with three subplots (one row, three columns)
fig, ax = plt.subplots(1, 3, figsize=(18, 6))  # Adjust the figure size as needed

# Plot for Residuals
zone.plot(column='residuals', cmap='viridis', ax=ax[0], legend=True)
ax[0].set_title('SAR Model Residuals')
ax[0].set_axis_off()

# Plot for Rent Effect
zone.plot(column='Rent_Effect', cmap='viridis', ax=ax[1], legend=True, legend_kwds={'label': "Effect on Rent"})
zone[~zone['Significant_Rent']].plot(color='lightgrey', ax=ax[1])
ax[1].set_title('Effect of rule breaking Airbnbs on rent')
ax[1].set_axis_off()

# Plot for Vacancy Effect
zone.plot(column='Vacancy_Effect', cmap='viridis', ax=ax[2], legend=True, legend_kwds={'label': "Effect on Vacancy"})
zone[~zone['Significant_Vacancy']].plot(color='lightgrey', ax=ax[2])
ax[2].set_title('Effect of rule breaking Airbnbs on vacancy')
ax[2].set_axis_off()

#output
plt.savefig('plots/Results_of_SAR_and_GWR_model.png', dpi=300, bbox_inches='tight')  

# Adjust layout
plt.tight_layout()
```

### Figure 4: Results of SAR and GWR Analysis of the Effect of Rule-Breaking Airbnbs on Monthly Rent and Vacancy Ratio

![](plots/Results_of_SAR_and_GWR_model.png)

From the analysis above, enforcing the 90-day policy is essential to address rising rents, increasing vacancy rates, and spatial inequality driven by short-term rentals, thereby preserving housing affordability and community stability in affected hotspots.

---

### Logistic Regression: Factors Drive Policy Violations
To explore the relationship in the study area between variables and their spatial distribution characteristics, we adopted logistics regression model to establish the model and evaluated the performance of the model through the residual and spatial autocorrelation test.

```{python}
#| echo: false
# Imports  
import pandas as pd  
import numpy as np  
import matplotlib.pyplot as plt  
import seaborn as sns  
from scipy import stats  
from statsmodels.tools import add_constant  
from statsmodels.stats.outliers_influence import variance_inflation_factor  
import statsmodels.api as sm  
from shapely.geometry import Point  
from libpysal.weights import KNN  
from esda.moran import Moran  
from splot.esda import moran_scatterplot  
import geopandas as gpd  
from pandas.plotting import table  
import matplotlib.gridspec as gridspec  
```

```{python}
#| echo: false
# Step 2: Load and preprocess data   
regression_data = airbnb_data

# Select and clean columns  
columns_to_keep = [  
    "estimated_nights_booked",   
    "room_type",   
    "price",   
    "minimum_nights",  
    "calculated_host_listings_count",   
    "longitude",   
    "latitude"  
]  
regression_data = regression_data[columns_to_keep].dropna()  

# Create binary outcome for nights booked  
threshold = regression_data["estimated_nights_booked"].median()  
regression_data["estimated_nights_booked_binary"] = (  
    regression_data["estimated_nights_booked"] > threshold  
).astype(int)  

# Encode 'room_type' using OneHotEncoder  
from sklearn.preprocessing import OneHotEncoder  
encoder = OneHotEncoder(drop="first", sparse_output=False)  
room_type_encoded = encoder.fit_transform(regression_data[["room_type"]])  
room_type_columns = encoder.get_feature_names_out(["room_type"])  
room_type_df = pd.DataFrame(room_type_encoded, columns=room_type_columns, index=regression_data.index)  

# Add multi-list host column  
regression_data["multi_list_host"] = (  
    regression_data["calculated_host_listings_count"] > 1  
).astype(int)  

# Drop columns no longer needed and combine encoded features  
regression_data = regression_data.drop(columns=["calculated_host_listings_count", "room_type"])  
regression_data = pd.concat([regression_data, room_type_df], axis=1)  
```

```{python}
#| echo: false
#| output: false
#| include: false

# Step 3: VIF Calculation  
X = regression_data.drop(  
    columns=["estimated_nights_booked", "estimated_nights_booked_binary", "longitude", "latitude"]  
)  
X = add_constant(X)  

# Calculate VIF  
vif_data = pd.DataFrame()  
vif_data["Variable"] = X.columns  
vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]  

vif_results = vif_data  
```

```{python}
#| echo: false
#| output: false
#| include: false
# Step 4: Run Logistic Regression  
Y = regression_data["estimated_nights_booked_binary"]  
logit_model = sm.Logit(Y, X)  
result = logit_model.fit()  

# Extract and display summary table  
regression_summary = result.summary().tables[1]  
data = regression_summary.data[1:]  # Skip the header row  
headers = regression_summary.data[0]  # Use the header  
summary_df = pd.DataFrame(data, columns=headers)  

# Add predicted probabilities and residuals  
regression_data["predicted_probs"] = result.predict(X)  
regression_data["residuals"] = (  
    regression_data["estimated_nights_booked_binary"] - regression_data["predicted_probs"]  
)  

print(result.summary())  
print("Logistic regression complete.")
```

```{python}
#| echo: false
#| output: false
#| include: false
# Step 1: Clip predicted probabilities  
regression_data["predicted_probs"] = regression_data["predicted_probs"].clip(1e-6, 1 - 1e-6)  

# Step 2: Calculate Deviance Residuals  
eps = 1e-6  # Small constant to avoid numerical issues  
regression_data["deviance_residuals"] = np.sign(  
    regression_data["estimated_nights_booked_binary"] - regression_data["predicted_probs"]  
) * np.sqrt(  
    -2 * (  
        regression_data["estimated_nights_booked_binary"] * np.log(regression_data["predicted_probs"] + eps) +  
        (1 - regression_data["estimated_nights_booked_binary"]) * np.log(1 - regression_data["predicted_probs"] + eps)  
    )  
)  

# Step 3: Convert data into GeoDataFrame  
regression_data["geometry"] = [  
    Point(xy) for xy in zip(regression_data["longitude"], regression_data["latitude"])  
]  
gdf = gpd.GeoDataFrame(regression_data, geometry="geometry", crs="EPSG:4326")  

# Step 4: Create KNN weight matrix  
weights = KNN.from_dataframe(gdf, k=3)  # k=3 means each point will connect to its 3 nearest neighbors  

# Step 5: Check for isolated points in the weight matrix  
islands = weights.islands  # Return a list of indices that are isolated/disconnected  

if len(islands) > 0:  
    print(f"Found {len(islands)} isolated points. Removing and recalculating weights...")  
    gdf_filtered = gdf.drop(islands, axis=0)  # Drop the rows corresponding to isolated points  
    weights = KNN.from_dataframe(gdf_filtered, k=3)  # Recalculate weights for the filtered dataset  
else:  
    print("No isolated points found. Continuing with the original dataset.")  
    gdf_filtered = gdf  # If no isolated points, the original dataset is used  

# Step 6: Calculate Moran's I  
moran = Moran(gdf_filtered["deviance_residuals"], weights)  

# Print Moran's I results  
print("Moran's I Results:")  
print(f"Moran's I: {moran.I:.4f}")  
print(f"p-value: {moran.p_sim:.4f}")  
print(f"z-score: {moran.z_sim:.4f}")  
```

```{python}
#| echo: false
# Create a 3x2 subplot layout  
fig = plt.figure(figsize=(15, 18))  

# Add main title  
plt.suptitle("Figure 5. Visualisation of Residuals and Moran's I",   
             fontsize=16,   
             fontweight='bold',   
             y=0.95)  # Adjust y position of the title  

# Residuals vs Fitted Values  
ax1 = plt.subplot(3, 2, 1)  
sns.scatterplot(x=gdf_filtered["predicted_probs"], y=gdf_filtered["deviance_residuals"],  
                ax=ax1, color="blue", s=25)  
ax1.axhline(y=0, color="red", linestyle="--", linewidth=1)  
ax1.set_title("Residuals vs Fitted (Deviance Residuals)")  
ax1.set_xlabel("Fitted values / Predicted Probabilities")  
ax1.set_ylabel("Deviance Residuals")  

# Q-Q Plot  
ax2 = plt.subplot(3, 2, 2)  
sm.qqplot(gdf_filtered["deviance_residuals"], line="45", ax=ax2)  
ax2.set_title("Q-Q Plot (Deviance Residuals)")  

# Scale-Location Plot  
ax3 = plt.subplot(3, 2, 3)  
sns.scatterplot(x=gdf_filtered["predicted_probs"],   
                y=np.sqrt(np.abs(gdf_filtered["deviance_residuals"])),  
                ax=ax3, color="blue", s=25)  
ax3.axhline(y=0, color="red", linestyle="--", linewidth=1)  
ax3.set_title("Scale-Location (√|Deviance Residuals|)")  
ax3.set_xlabel("Fitted values / Predicted Probabilities")  
ax3.set_ylabel("√|Deviance Residuals|")  

# Residuals vs Leverage  
ax4 = plt.subplot(3, 2, 4)  
try:  
    influence = result.get_influence()  
    leverage = influence.hat_matrix_diag  
    deviance_residuals = gdf_filtered["deviance_residuals"]  
    
    ax4.scatter(leverage, deviance_residuals ** 2, alpha=0.5, color="blue", s=25)  
    ax4.axhline(y=0, color="red", linestyle="--", linewidth=1)  
    ax4.set_title("Residuals vs Leverage (Deviance Residuals)")  
    ax4.set_xlabel("Leverage")  
    ax4.set_ylabel("Deviance Residuals^2")  
except Exception as e:  
    print("Error during Residuals vs Leverage plotting:", e)  

# Moran's I Scatterplot  
ax5 = plt.subplot(3, 2, (5, 6))  # Span both columns in the last row  
moran_scatterplot(moran, ax=ax5)  
ax5.set_title("Moran's I Scatterplot (Deviance Residuals)", fontsize=14)  

# Adjust layout with top margin for main title  
plt.tight_layout(rect=[0, 0, 1, 0.95])  # Leave space at top for suptitle  
plt.show()
```

# Missing Variables and Spatial Patterns?  

Key property and host characteristics were identified as significant drivers of policy violations (p < 0.001). However, the low pseudo R-squared (0.05) suggests the model explains only a small portion of the causes.  

Residual analysis and Moran's I test (I = 0.0855, p = 0.001) highlighted weak but significant spatial patterns, indicating missing factors and spatial heterogeneity. Further model improvements are needed to better capture these complexities.

---

## Geographically Weighted Regression

Research shows that factors affecting Airbnb distribution in London vary across areas. To address this spatial heterogeneity, the Geographically Weighted Regression (GWR) model will be used, as it allows variables’ coefficients to change with location. Meanwhile, previous studies have shown a strong association between public transport density, green space density, and tourist attraction density with Airbnb distribution [@xu_influence_2020]. This study incorporates these variables into the model to examine whether they similarly influence the occurrence of non-compliant listings. The data for these variables were obtained from OpenStreetMap.

```{python}
#| echo: false
#| warning: false
#| output: false

# Step 1: Import Required Libraries
import geopandas as gpd
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from matplotlib.font_manager import FontProperties
from scipy.stats import chi2_contingency, pearsonr, pointbiserialr
from shapely.geometry import Point
from mgwr.gwr import GWR, MGWR
from mgwr.sel_bw import Sel_BW
from shapely.geometry import Point
```

```{python}
#| echo: false
#| warning: false
#| output: false

# Step 2: Load Data
## 2.1. Load Airbnb listings data
listing = pd.read_csv(
    "data/processed_airbnb_data.csv"
)  # Estimated nights of Airbnb stays

## 2.2. Load London ward boundaries and reproject to EPSG:27700
ward = gpd.read_file(
    "data/statistical-gis-boundaries-london/ESRI/London_Ward_CityMerged.shp"
).to_crs(epsg=27700)

## 2.3. Load points of interest (POIs) and reproject
nature = gpd.read_file("data/ldn_poi/gis_osm_natural_free_1.shp").to_crs(
    epsg=27700
)  # Nature POIs
transport = gpd.read_file("data/ldn_poi/gis_osm_transport_free_1.shp").to_crs(
    epsg=27700
)  # Transport POIs
tourism = gpd.read_file("data/ldn_poi/gis_osm_pois_free_1.shp").to_crs(
    epsg=27700
)  # Tourism POIs
```

```{python}
#| echo: false
#| warning: false
#| output: false

# Step 3: Convert Listings Data to GeoDataFrame
## 3.1. Convert CSV longitude/latitude data into a GeoDataFrame
geometry = [
    Point(xy) for xy in zip(listing["longitude"], listing["latitude"])
]  # Create Point geometries
listing_spatial = gpd.GeoDataFrame(
    listing, geometry=geometry, crs="EPSG:4326"
)  # Set GeoDataFrame with CRS

## 3.2. Reproject to British National Grid (EPSG:27700)
listing_spatial = listing_spatial.to_crs(epsg=27700)
```

```{python}
#| echo: false
#| warning: false
#| output: false

# Step 4: Calculate Density of Listings > 90 Nights per Ward (Y)
## 4.1. Filter listings where estimated nights are >= 90
listing_spatial_90 = listing_spatial[listing_spatial["estimated_nights_booked"] >= 90]

## 4.2. Spatial join: count listings per ward
join_listing_ward = gpd.sjoin(
    ward, listing_spatial_90, how="left", predicate="intersects"
)
ward_count = (
    join_listing_ward.groupby("GSS_CODE").size().reset_index(name="listing_count")
)

## 4.3. Merge counts into ward dataset
ward = ward.merge(ward_count, on="GSS_CODE", how="left").fillna({"listing_count": 0})

## 4.4. Calculate area and listing density
ward["area"] = ward.geometry.area
ward["listing_density"] = ward["listing_count"] / ward["area"]

## 4.5. Plot the density results
fig, ax = plt.subplots(1, 1, figsize=(10, 8))
ward.plot(column="listing_density", cmap="viridis", legend=True, ax=ax)
ax.set_title("Density of Over-90-Days Listings by Ward")
plt.axis("off")
plt.show()
```

```{python}
#| echo: false
#| warning: false
#| output: false

# Step 5: Calculate Density of Green Spaces, Transport, and Tourism POIs
# --------------------------------------------------------------
## 5.1 Density of Green Spaces (X1)
### 5.1.1. Spatial join: green spaces per ward
join_nature_ward = gpd.sjoin(ward, nature, how="left", predicate="intersects")
nature_count = (
    join_nature_ward.groupby("GSS_CODE").size().reset_index(name="nature_count")
)
ward = ward.merge(nature_count, on="GSS_CODE", how="left").fillna({"nature_count": 0})

### 5.1.2. Calculate green space density
ward["nature_density"] = ward["nature_count"] / ward["area"]

### 5.1.3. Plot the results
fig, ax = plt.subplots(1, 1, figsize=(10, 8))
ward.plot(column="nature_density", cmap="viridis", legend=True, ax=ax)
ax.set_title("Density of Green Spaces by Ward")
plt.axis("off")
plt.show()

# --------------------------------------------------------------
## 5.2 Density of Transport Networks (X2)
### 5.2.1. Spatial join: transport networks per ward
join_transport_ward = gpd.sjoin(ward, transport, how="left", predicate="intersects")
transport_count = (
    join_transport_ward.groupby("GSS_CODE").size().reset_index(name="transport_count")
)
ward = ward.merge(transport_count, on="GSS_CODE", how="left").fillna(
    {"transport_count": 0}
)

### 5.2.2. Calculate transport network density
ward["transport_density"] = ward["transport_count"] / ward["area"]

### 5.2.3. Plot the results
fig, ax = plt.subplots(1, 1, figsize=(10, 8))
ward.plot(column="transport_density", cmap="viridis", legend=True, ax=ax)
ax.set_title("Density of Transport Networks by Ward")
plt.axis("off")
plt.show()

# --------------------------------------------------------------
## 5.3 Density of Tourism Sites (X3)
### 5.3.1 Extract poi related to tourist attractions
tourism = tourism[
    tourism["code"].astype(str).str.startswith("26")
]  # In OSM, places beginning with 26 represent tourist attractions

### 5.3.2 Spatial join: tourism sites per ward
join_tourism_ward = gpd.sjoin(ward, tourism, how="left", predicate="intersects")
ward_count = (
    join_tourism_ward.groupby("GSS_CODE").size().reset_index(name="tourism_count")
)
ward = ward.merge(ward_count, on="GSS_CODE", how="left")
ward["tourism_count"] = ward["tourism_count"].fillna(0)

### 5.3.3. Calculate tourism site density
ward["tourism_density"] = ward["tourism_count"] / ward["area"]

### 5.3.4 Plot the results
fig, ax = plt.subplots(1, 1, figsize=(10, 8))
ward.plot(column="tourism_density", cmap="viridis", legend=True, ax=ax)
ax.set_title("Density of Tourism Attractions by Ward")
plt.axis("off")
plt.show()
```

```{python}
#| echo: false
#| warning: false
#| output: false

# Step 6: Export Results to Shapefile
## 6.1. Keep required columns and geometry
columns_to_keep = ["listing_density", "nature_density", "transport_density", "geometry"]
final_data = ward[columns_to_keep]

## 6.2. Save to a new Shapefile
final_data.to_file("data/final_results.shp", driver="ESRI Shapefile")
```

```{python}
#| echo: false
#| warning: false
#| output: false

# Step 7: Spatial Join and Filtering Airbnb Data
## 7.1. Perform a spatial join: Airbnb data with ward boundaries
listing_ward = gpd.sjoin(listing_spatial, ward, how="left", predicate="intersects")

## 7.2. Select relevant columns to keep
columns_to_keep = [
    "id",
    "room_type",
    "price",
    "minimum_nights",
    "calculated_host_listings_count",
    "GSS_CODE",
    "geometry",
]
listing_ward = listing_ward[columns_to_keep]

## 7.3. Preview the joined data
print(listing_ward.head())

## 7.4. Save the spatially joined data to a Shapefile
listing_ward.to_file("data/listing_ward.shp", driver="ESRI Shapefile")
```

```{python}
#| echo: false
#| warning: false
#| output: false

# Step 8: Calculate Average Price and Minimum Nights per Ward
# --------------------------------------------------------------
## 8.1 Calculate Average Price
### 8.1.1 Group by GSS_CODE and calculate the average price
avg_price = listing_ward.groupby("GSS_CODE", as_index=False)["price"].mean()
avg_price.rename(columns={"price": "avg_price"}, inplace=True)

### 8.1.2. Merge the average price back to the ward data
ward["avg_price"] = ward["GSS_CODE"].map(avg_price.set_index("GSS_CODE")["avg_price"])

### 8.1.3. Check the updated ward data
print(ward.info())

# --------------------------------------------------------------
## 8.2 Calculate Average Minimum Nights
### 8.2.1 Group by GSS_CODE and calculate the average minimum nights
avg_night = listing_ward.groupby("GSS_CODE", as_index=False)["minimum_nights"].mean()
avg_night.rename(columns={"minimum_nights": "avg_night"}, inplace=True)

### 8.2.2 Merge the average minimum nights back to the ward data
ward["avg_night"] = ward["GSS_CODE"].map(avg_night.set_index("GSS_CODE")["avg_night"])

### 8.2.3 Check the updated ward data
print(ward.info())
```

```{python}
#| echo: false
#| warning: false
#| output: false

# Step 9: Calculate Average Host Listings and Entire Home Density
# --------------------------------------------------------------
## 9.1 Average Host Listings
### 9.1.1 Calculate the average number of host listings per ward
avg_host_listing = listing_ward.groupby("GSS_CODE", as_index=False)[
    "calculated_host_listings_count"
].mean()
avg_host_listing.rename(
    columns={"calculated_host_listings_count": "avg_host_listing"}, inplace=True
)

### 9.1.2. Merge the average host listings back to the ward data
ward["avg_host_listing"] = ward["GSS_CODE"].map(
    avg_host_listing.set_index("GSS_CODE")["avg_host_listing"]
)

### 9.1.3. Check the updated ward data
print(ward.info())

# --------------------------------------------------------------
## 9.2 Calculate 'Entire Home' Density
### 9.2.1 Filter rows where room_type contains 'Entire'
entire_count = listing_ward[listing_ward["room_type"].str.contains("Entire")]
entire_count = entire_count.groupby("GSS_CODE").size().reset_index(name="entire_count")

### 9.2.2 Merge the count of 'Entire' listings back to the ward data
ward = ward.merge(entire_count, on="GSS_CODE", how="left")

### 9.2.3 Calculate the density of 'Entire' listings per ward area
ward["entire_density"] = ward["entire_count"] / ward["area"]

### 9.2.4 Fill NaN values for wards with no 'Entire' listings
ward["entire_count"] = ward["entire_count"].fillna(0)
ward["entire_density"] = ward["entire_density"].fillna(0)

### 9.2.5. Check the updated ward data
print(ward.info())
```

```{python}
#| echo: false
#| warning: false
#| output: false

# Step 10: Save Results to Shapefile
## 10.1. Define the columns to retain
columns_to_keep = [
    "listing_density",
    "nature_density",
    "transport_density",
    "tourism_density",
    "avg_price",
    "avg_night",
    "avg_host_listing",
    "entire_density",
]

## 10.2 Create a new dataset with selected columns and save as Shapefile
gwr_data = ward[columns_to_keep + ["geometry"]]
gwr_data.to_file("data/gwr.shp", driver="ESRI Shapefile")
```

```{python}
#| echo: false
#| warning: false
#| output: false

# Step 11: Perform GWR
# --------------------------------------------------------------
## 11.1 Prepare GWR Input Data
### 11.1.1 Read the Shapefile data for GWR
gwr = gpd.read_file("data/gwr.shp")

### 11.1.2 Extract centroids and coordinates for GWR
gwr["geometry"] = gwr.centroid
gwr["x"] = gwr.geometry.x
gwr["y"] = gwr.geometry.y

### 11.1.3 Define dependent variable (Y), independent variables (X), and coordinates
Y = gwr[["listing_de"]].values
X = gwr[
    [
        "nature_den",
        "transport_",
        "tourism_de",
        "avg_price",
        "avg_night",
        "avg_host_l",
        "entire_den",
    ]
].values
coords = gwr[["x", "y"]].values

# --------------------------------------------------------------
## 11.2 Run GWR Model
### 11.2.1 Determine the optimal bandwidth
bw = Sel_BW(coords, Y, X).search()

### 11.2.2 Fit the GWR model
model = GWR(coords, Y, X, bw)
results = model.fit()

### 11.2.3 Print summary of GWR results
print("GWR Results Summary:")
print(f"R²: {results.R2}")
print(f"AIC: {results.aic}")
print(f"Bandwidth: {bw}")
print(f"Parameters shape: {results.params.shape}")

# --------------------------------------------------------------
## 11.3 Extract Local R² and Coefficients
gwr['gwr_r2'] = results.localR2

for i, col in enumerate(['intercept', 'nature_density', 'transport_density', 'tourism_density',
                         'avg_price', 'avg_night', 'avg_host_listing', 'entire_density']):
    gwr[f'gwr_{col}'] = results.params[:, i]
```

```{python}
#| echo: false
#| warning: false

# Step 12: Visualisation
## 12.1. Set up the figure with a 2x4 layout
fig, axes = plt.subplots(2, 4, figsize=(16, 8))  # 2 rows, 4 columns

## 12.2. Define the variables and custom titles for each subplot
variables = [
    'gwr_r2', 'transport_density', 'tourism_density', 'nature_density',
    'avg_price', 'avg_night', 'avg_host_listing', 'entire_density'
]

titles = [
    'Local R² Distribution', 'Transport Density Coefficient', 'Tourism Density Coefficient', 'Nature Density Coefficient',
    'Average Price Coefficient', 'Average Minimumn Nights Coefficient', 'Average Host Listings Coefficient',
    'Entire Home Density Coefficient'
]

## 12.3. Loop through the variables and plot each on the corresponding axis
for i, (var, title) in enumerate(zip(variables, titles)):
    row, col = divmod(i, 4)  # Calculate row and column index for 2x4 layout
    gwr.plot(column=f"gwr_{var}" if var != "gwr_r2" else var, 
             cmap='viridis', legend=True, ax=axes[row, col])  # Use viridis colormap
    axes[row, col].set_title(title)  # Set custom title
    axes[row, col].axis('off')  # Remove axes for clean plots

## 12.4. Adjust layout and display the figure
plt.tight_layout()
plt.show()
```

The GWR model shows high explanatory power, with R-squared values exceeding 0.8 across most of London and reaching 0.95 in central areas where non-compliant listings are concentrated. This confirms the strong relevance of the selected variables. However, significant spatial heterogeneity is observed, indicating that the variables cannot uniformly explain rule-breaking Airbnb listings. For public transport density, the lower-left central region shows a strong positive correlation, while the upper-right exhibits a moderate negative correlation. In contrast, for tourist attraction density, the lower left shows a strong negative correlation, whereas the upper-right demonstrates a moderate positive correlation.

--- 

## Discussion
We conducted a spatial analysis based on Inside Airbnb data and found that illegal listings in London exhibit a clear clustering pattern and have a significant impact on local housing market rental levels and vacancy rates. This suggests that the current 90-day short-term rental policy is necessary and reasonable. Further multi-factor analysis shows that although there is a certain correlation between illegal listings and factors such as host type, property type, minimum rental period, and price, their impact is limited. In contrast, traffic network density and tourist density are the key driving factors for the distribution of illegal listings. However, the direction and degree of their influence vary significantly among different regions, indicating that a single regulatory policy is difficult to effectively solve all illegal activities in all regions.

Therefore, we suggest that the government should establish a citywide housing database and require short-term rental platforms to sign data sharing agreements with the government, regularly uploading housing information and usage data to support regulatory work. At the same time, we can learn from the experience of European cities such as Barcelona and force short-term rental platforms to remove unregistered listings and impose stricter penalties on platform violations. （Ref 1, https://www.sciencedirect.com/science/article/pii/S0264275124008175）
Combining London's current Housing Act 2015 (Ref 2), we further suggest implementing differential regulation by region. Based on data analysis, the government can allocate more enforcement resources to hotspot areas of illegal listings and optimise patrol and rectification measures, thereby improving regulatory efficiency and reducing the occurrence of illegal activities.

--- 

## References
