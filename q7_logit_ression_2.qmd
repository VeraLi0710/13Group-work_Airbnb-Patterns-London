---
title: 'Q7: Identifying Factors Associated with Airbnb Listings Exceeding 90 Days of Occupancy'
jupyter:
  jupytext:
    text_representation:
      extension: .qmd
      format_name: quarto
      format_version: '1.0'
      jupytext_version: 1.16.4
  kernelspec:
    display_name: Python (base)
    language: python
    name: base
---

### Logistic regression analysis and verify

```{python}
#| echo: false
# packages
import pandas as pd  
import numpy as np  
from scipy.stats import pearsonr, pointbiserialr, chi2_contingency  
import matplotlib.pyplot as plt  
from matplotlib.font_manager import FontProperties  
import seaborn as sns 
import statsmodels.api as sm  
from statsmodels.stats.outliers_influence import variance_inflation_factor  
from statsmodels.tools.tools import add_constant  
from sklearn.preprocessing import OneHotEncoder  
from scipy import stats
import geopandas as gpd  
from libpysal.weights import DistanceBand  
from esda import Moran
from splot.esda import moran_scatterplot  
from libpysal.weights import KNN 
```

```{python}
#| '0': e
#| '1': c
#| '2': h
#| '3': o
#| '4': ':'
#| '5': f
#| '6': a
#| '7': l
#| '8': s
#| '9': e
# Load the dataset  
file_path = 'processed_airbnb_data.csv'  
regression_data = pd.read_csv(file_path)  

# Select columns to keep  
columns_to_keep = [  
    'estimated_nights_booked',       
    'room_type',                     
    'price',                         
    'minimum_nights',                
    'calculated_host_listings_count',  
    'longitude',  
    'latitude'  
]  

# Print dataset information to verify  
print(regression_data.info())  

# Filter relevant columns and drop missing values  
regression_data = regression_data[columns_to_keep]  
regression_data = regression_data.dropna()  

# Convert 'estimated_nights_booked' to binary  
threshold = regression_data['estimated_nights_booked'].median()  # You can also use mean  
regression_data['estimated_nights_booked_binary'] = (  
    regression_data['estimated_nights_booked'] > threshold  
).astype(int)  

# Encode the categorical variable 'room_type' using one-hot encoding  
from sklearn.preprocessing import OneHotEncoder  
encoder = OneHotEncoder(drop='first', sparse_output=False)  # Use sparse_output=False for dense array  
room_type_encoded = encoder.fit_transform(regression_data[['room_type']])  
room_type_columns = encoder.get_feature_names_out(['room_type'])  
room_type_df = pd.DataFrame(room_type_encoded, columns=room_type_columns, index=regression_data.index)  

# Create binary variables for single-list host and multi-list host  
regression_data['single_list_host'] = (  
    regression_data['calculated_host_listings_count'] == 1  
).astype(int)  # 1 if single-list host, 0 otherwise  
regression_data['multi_list_host'] = (  
    regression_data['calculated_host_listings_count'] > 1  
).astype(int)  # 1 if multi-list host, 0 otherwise  

# Drop one of the collinear variables  
regression_data = regression_data.drop(columns=['single_list_host'])  
regression_data = regression_data.drop(columns=['calculated_host_listings_count'])  

# Add the encoded variables back to the dataset  
regression_data = pd.concat([regression_data, room_type_df], axis=1)  
regression_data = regression_data.drop(columns=['room_type'])  # Drop the original categorical column  

# VIF

# Prepare the independent variables for VIF calculation  
X = regression_data.drop(  
    columns=['estimated_nights_booked',   
            'estimated_nights_booked_binary',  
            'longitude',  
            'latitude']  
)  # Exclude dependent variables and geographical coordinates  
X = add_constant(X)  # Add constant for VIF calculation  


# Calculate VIF for each variable  
vif_data = pd.DataFrame()  
vif_data['Variable'] = X.columns  
vif_data['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]  
print("Variance Inflation Factor (VIF):")  
print(vif_data)  

# Correlation Matrix  
print("\nCorrelation Matrix:")  
correlation_matrix = X.corr()  
print(correlation_matrix)  

# Logistic regression
# Fit logistic regression  
Y = regression_data['estimated_nights_booked_binary']  

# Try fitting with different optimization methods  
try:  
    logit_model = sm.Logit(Y, X)  
    result = logit_model.fit(method='newton', maxiter=100)  
except:  
    try:  
        result = logit_model.fit(method='bfgs', maxiter=100)  
    except:  
        result = logit_model.fit(method='lbfgs', maxiter=100)  

print("\nLogistic Regression Results:")  
print(result.summary())  

# Save important metrics  
print("\nModel Performance Metrics:")  
print(f"Pseudo R-squared: {result.prsquared:.4f}")  
print(f"Log-Likelihood: {result.llf:.4f}")  
print(f"AIC: {result.aic:.4f}")  

# Print odds ratios  
print("\nOdds Ratios:")  
odds_ratios = np.exp(result.params)  
conf_int = np.exp(result.conf_int())  
odds_ratios_summary = pd.DataFrame({  
    'Odds Ratio': odds_ratios,  
    'Lower CI': conf_int[0],  
    'Upper CI': conf_int[1]  
})  
print(odds_ratios_summary)

# Residual Calculation  
predicted_probs = result.predict()  
residuals = Y - predicted_probs  

# Create a DataFrame for residual analysis  
regression_data = pd.DataFrame({  
    'latitude': regression_data['latitude'],  # Use latitude from the dataset  
    'longitude': regression_data['longitude'],  # Use longitude from the dataset  
    'actual': Y,  # Actual binary outcomes  
    'predicted_probs': predicted_probs  # Predicted probabilities  
})  

# Calculate residuals  
regression_data['residuals'] = regression_data['actual'] - regression_data['predicted_probs']  

# Residual stats  
residuals_stats = {  
    'Mean': np.mean(residuals),  
    'Std Dev': np.std(residuals),  
    'Min': np.min(residuals),  
    'Max': np.max(residuals),  
    'Skewness': stats.skew(residuals),  
    'Kurtosis': stats.kurtosis(residuals)  
}  

print("\nResiduals Statistics:")  
for stat, value in residuals_stats.items():  
    print(f"{stat}: {value:.4f}")  

# Visualisations  
## 2. Basic Residual Plot  
plt.figure(figsize=(10, 6))  
plt.scatter(predicted_probs, residuals, alpha=0.5)  
plt.axhline(y=0, color='r', linestyle='--')  
plt.xlabel('Predicted Probabilities')  
plt.ylabel('Residuals')  
plt.title('Residual Plot')  
plt.show()  

## 3. Barchart  
plt.figure(figsize=(10, 6))  
sns.histplot(residuals, kde=True)  
plt.xlabel('Residuals')  
plt.ylabel('Count')  
plt.title('Distribution of Residuals')  
plt.show()  

## 4. Q-Q Plot  
plt.figure(figsize=(10, 6))  
stats.probplot(residuals, dist="norm", plot=plt)  
plt.title('Q-Q Plot of Residuals')  
plt.show()  
```

```{python}
# 继续之前的代码  

from libpysal.weights import KNN, DistanceBand  
from esda.moran import Moran  
from splot.esda import moran_scatterplot  
import geopandas as gpd  
from shapely.geometry import Point  
import matplotlib.pyplot as plt  

# 1. 构建 GeoDataFrame（经纬度和残差已经存在于数据集中）  
regression_data['geometry'] = [  
    Point(xy) for xy in zip(regression_data['longitude'], regression_data['latitude'])  
]  
gdf = gpd.GeoDataFrame(regression_data, geometry='geometry', crs="EPSG:27700")  

# 2. 构建空间权重矩阵  
# 使用基于 k=5 最近邻的方法创建权重矩阵  
k = 1
weights = KNN.from_dataframe(gdf, k=k)  

# 检查权重矩阵是否正确  
print(f"Number of neighbors for each point: {weights.n}")  

# 3. 计算莫兰指数（基于残差）  
moran = Moran(gdf['residuals'], weights)  

# 打印莫兰指数结果  
print(f"\nMoran's I: {moran.I:.4f}")  
print(f"P-value: {moran.p_sim:.4f}")  
print(f"Z-Score: {moran.z_sim:.4f}")  

# 4. 绘制莫兰散点图  
fig, ax = moran_scatterplot(moran)  
ax.set_title(f"Moran's I Scatterplot (k = {k})")  
plt.show()  

# 5. 可视化残差的空间分布  
plt.figure(figsize=(12, 8))  
gdf.plot(column='residuals', cmap='coolwarm', legend=True, markersize=10)  
plt.title("Spatial Distribution of Residuals")  
plt.xlabel("Longitude")  
plt.ylabel("Latitude")  
plt.show()
```

```{python}
### Result
Moran' I 显示残差的空间相关性存在，说明模型可能遗漏了一些应变量的空间特性从，这说明模型需要进一步优化，将通过进一步的 GWLR进行分析。
```
